= API 2 Model

== Current Approach

Our current approach relies on buffering the graph data into local structures. We've got
different implementations, one which consumes more memory but allows fast
iteration on the Graph. The other one has a more flexible memory model but performs
not as good as the heavy one. Both versions take a good amount of time to load from
the neo4j graph.

During the development of algorithms we have seen that not every algorithm needs the whole
bunch of data.
But since we always load the whole graph we waste some time with unnecessary imports.

[ditaa]
----

    +-----------+
    |           | import  +-------+ computation +--------+
    |   Neo4j{s}|-------->| Graph |------------>| Result |
    +-----------+         +-------+             +--------+

----


== New Approach

Loading the data takes a lot of time. If we just split the Graph into several interfaces and
load only the needed ones, we end up with less import time. This approach also allows us to
define several implementations for an interface. E.g. a SingleRunNodeIterator which is designed
to run only once (and therefore does not need to buffer all nodes). It would also ease
the implementation of new operations like a concurrentNodeIterator (to pick batches of nodes
for parallel evaluation) or an AllRelationsIterator (which doesn't need a node-id to start from).

[ditaa]
----
                            +---------+
                            |         |
                            |  Neo4j  |
                            |    {s}  |
                            +----+----+
                                 ^
                                 |
        +-------------+----------+------+--------+--------- ...
        |             |          |               |
    +---+------+ +----+---+ +----+----+ +--------+--------+
    | NodeIter | | Degree | | RelIter | | WeightedRelIter | ...
    +---+------+ +----+---+ +----+----+ +--------+--------+
        ^             ^          ^               ^
        |             |          |               |
        +------+------+          |               |
               :                 :               :
         +-----+-----+     +-----+-----+   +-----+-----+
         |           |     |           |   |           |
         |  PageRank |     | UnionFind |   | MinSTree  |
         |    {c}    |     |     {c}   |   |   {c}     |
         +-----------+     +-----------+   +-----+-----+
                                                 ^
                                                 |
                                                 :
                                           +-----+-----+
                                           |           |
                                           |   kMeans  |
                                           |    {c}    |
                                           +-----------+
----

The Diagram shows the basic Idea. Each Algorithm needs a set of data sources. PageRank for example
relies only on nodeIds and their degrees while a UnionFindStruct can be built just by iterating
over all relations (or weighted rels. depends on use case). kMeans on the other hand needs the result
of a preceding algorithm to work.

We can think of an algorithm as a component which needs several data sources as input and also is a
data source by itself. The topmost sources may work directly with the neo4j core api while the lower
ones need precomputed data from previous steps.

== Implementation

We know every Algorithm needs one or more data sources to work. The algorithm itself produces a result
which might act as an input for the next one. Each algorithm also may need some kind of A Priori knowledge
or configuration settings which could be built with a String-to-Object map.

Implementing this concept might be done using standard java oop. The Constructor of the implementation
should take all needed data sources and a shared setupObject. Additional capabilities or requirements
could later be added with Annotations (e.g. parallel, singlethreaded, one-time-use, ..) and the actual
return type might just be the generic type of an interface.

Example algorithm interface::
```
public interface Algorithm<T> {
    public T compute();
}
```

Example datasource::
```
public interface NodeIterator {
    public void forEachNode(IntConsumer ic);
}

@Characteristics({ONE_TIME_USE})
public class SingleRunNodeIterator implements NodeIterator {
    public SingleRunNodeIterator(GraphDatabaseAPI api) { ... }
    public void forEachNode(IntConsumer ic) { ... }
}

```

Example algorithm::
```
@Characteristics({SINGLE_THREADED, ...})
public class PageRank implements Algorithm<PageRank.Result> {

    public PageRank(
        @Requirements(CONCURRENT) NodeIterator nodeIt,
        @Requirements(ONE_TIME_USE) Degrees degrees) {
            ...
    }

    public PageRank.Result compute();
}
```

Up to now I would suggest hardcoded chains of Algorithms and no Capabilities/Requirements.
The programmer can decide which implementation fits best for his algorithm. Only the Importer
(currently the GraphLoader) would check the constructor of the topmost algorithm to get a set
of data sources it needs. It then loads the appropriate sources and initializes the algorithm.

Later we might switch to some kind of object-graph-resolver which builds an optimal algorithm chain
for one use case.

== Conclusion

This approach offers several optimizations to the current impl.

- less refactoring when adding new operations
- lower loading time and less memory consumption for unused data
- several impls. of data sources with different characteristics (one-time-use, concurrent, ...)
- possibility to add an object-graph-resolver to build an optimal process chain
