= API 2 model

== Current approach

Our current approach relies on buffering the graph data into local structures.
We've got different implementations, one (HeavyGraph) which consumes more memory but allows fast iteration on the Graph.
The other one (LightGraph) has a more flexible memory model but performs not as well as the heavy one.
Both versions take some time to load from Neo4j, the HeavyGraph can be loaded in parallel.


During the development of algorithms we have seen that not every algorithm needs the whole graph data.
By default we currently load all aspects of the graph, that's why we waste some time with unnecessary imports.

[ditaa]
----

    +-----------+
    |           | import  +-------+ computation +--------+
    |   Neo4j{s}|-------->| Graph |------------>| Result |
    +-----------+         +-------+             +--------+

----


== New approach

Loading the data takes time.
If we split the Graph into several interfaces and load only the needed ones, we end up with less import time.
This approach also allows us to define several implementations for an interface.
E.g. a `SingleRunNodeIterator` which is designed to run only once (and therefore does not need to buffer at all).
It would also ease the implementation of new operations like a `ConcurrentNodeIterator` (to pick batches of nodes for parallel evaluation) or an `AllRelationsIterator` (which doesn't need a node-id to start from).

[ditaa]
----
                            +---------+
                            |         |
                            |  Neo4j  |
                            |    {s}  |
                            +----+----+
                                 ^
                                 |
        +-------------+----------+------+--------+--------- ...
        |             |          |               |
    +---+------+ +----+---+ +----+----+ +--------+--------+
    | NodeIter | | Degree | | RelIter | | WeightedRelIter | ...
    +---+------+ +----+---+ +----+----+ +--------+--------+
        ^             ^          ^               ^
        |             |          |               |
        +------+------+          |               |
               :                 :               :
         +-----+-----+     +-----+-----+   +-----+-----+
         |           |     |           |   |           |
         |  PageRank |     | UnionFind |   | MinSTree  |
         |    {c}    |     |     {c}   |   |   {c}     |
         +-----------+     +-----------+   +-----+-----+
                                                 ^
                                                 |
                                                 :
                                           +-----+-----+
                                           |           |
                                           |   kMeans  |
                                           |    {c}    |
                                           +-----------+
----

The Diagram shows the basic idea.
Each Algorithm needs a set of data sources.
_PageRank_ for example relies only on nodeIds and their degrees.
While the underlying structure for  _UnionFind_  can be built just by iterating over all relations (or weighted relationships. depends on use case).
_kMeans_ on the other hand needs the result of a preceding algorithm to work.

We can think of an algorithm as a component which needs several data sources as input and also is a
data source by itself. The topmost sources may work directly with the neo4j core api while the lower
ones need precomputed data from previous steps.

== Implementation

We know every Algorithm needs one or more data sources to work.
The algorithm itself produces a result which might act as an input for the next one.
Each algorithm also may need some kind of A Priori knowledge or configuration settings.

Implementing this concept might be done using standard java oop.
The constructor of the implementation should take all needed data sources and a shared setup object.
Additional capabilities or requirements could later be added with Annotations (e.g. parallel, singlethreaded, one-time-use, ..) and the actual return type might just be the generic type of an interface.

.Example algorithm interface:
----
public interface Algorithm<T> {
    public T compute();
}
----

.Example datasource:
----
public interface NodeIterator {
    public void forEachNode(IntConsumer ic);
}

@Characteristics({ONE_TIME_USE})
public class SingleRunNodeIterator implements NodeIterator {
    public SingleRunNodeIterator(GraphDatabaseAPI api) { ... }
    public void forEachNode(IntConsumer ic) { ... }
}
----

.Example algorithm
----
@Characteristics({SINGLE_THREADED, ...})
public class PageRank implements Algorithm<PageRank.Result> {

    public PageRank(
        @Requirements(CONCURRENT) NodeIterator nodeIt,
        @Requirements(ONE_TIME_USE) Degrees degrees) {
            ...
    }

    public PageRank.Result compute();
}
----

Up to now I would suggest hardcoded chains of Algorithms and no Capabilities/Requirements.
The programmer can decide which implementation fits best for his algorithm.
Only the Importer (currently the GraphLoader) would check the constructor of the topmost algorithm to get a set of data sources it needs.
It then loads the appropriate sources and initializes the algorithm.

Later we might switch to some kind of object-graph-resolver which builds an optimal algorithm chain for one use case.

== Conclusion

This approach offers several optimizations to the current implementation.

- less refactoring when adding new operations
- lower loading time and less memory consumption for unused data
- several impls. of data sources with different characteristics (one-time-use, concurrent, ...)
- possibility to add an object-graph-resolver to build an optimal process chain
