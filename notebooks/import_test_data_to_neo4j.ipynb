{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from py2neo import Graph, Node, Relationship \n",
    "\n",
    "graph = Graph('bolt://localhost:11038', auth=('neo4j', 'password'))\n",
    "\n",
    "graph.delete_all()\n",
    "\n",
    "n_nodes = graph.database.primitive_counts['NumberOfNodeIdsInUse']\n",
    "n_relationships = graph.database.primitive_counts['NumberOfRelationshipIdsInUse']\n",
    "print(\"Connected to graph database with {:,} nodes and {:,} relationships!\".format\n",
    "     (n_nodes, n_relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrain one id per Quanta\n",
    "print(\"Creating uniqueness constraint (and also index) on Quanta id's...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE CONSTRAINT ON (n:Quanta) ASSERT n.id IS UNIQUE;\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Constrain one name per author\n",
    "print(\"Creating uniqueness constraint (and also index) on Author names...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE CONSTRAINT ON (a:Author) ASSERT a.name IS UNIQUE;\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Constrain one name per organization\n",
    "print(\"Creating uniqueness constraint (and also index) on Organization names...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE CONSTRAINT ON (o:Organization) ASSERT o.name IS UNIQUE;\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add index for year of publication\n",
    "print(\"Creating index for publication year...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE INDEX ON :Quanta(year);\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Add index for language\n",
    "print(\"Creating index for langauge...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE INDEX ON :Quanta(lang);\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Add index for keywords HOW?\n",
    "print(\"Creating index for fos...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE INDEX ON :Quanta(fos);\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Add index for title (good idea?)\n",
    "print(\"Creating index for title...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE INDEX ON :Quanta(title);\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "CALL apoc.load.json('file:/Users/timholdsworth/code/scaling-science/notebooks/tmp/data/fake-data.txt') YIELD value AS q UNWIND q.id AS id UNWIND q.authors as authors\n",
    "MERGE (a:Author {name:authors.name})\n",
    "WITH a, q, authors\n",
    "WHERE authors.org is not null\n",
    "MERGE (i:Quanta {id:q.id}) ON CREATE SET i.year=q.year, i.title=q.title, i.fos=q.fos\n",
    "WITH q.id AS id, head(q.authors).name as firstName, last(q.authors).name as lastName, q.authors as authors\n",
    "UNWIND authors as author\n",
    "MATCH (i:Quanta {id:id}) \n",
    "MATCH (a:Author {name:author.name})\n",
    "WITH i, a, author.name = firstName as isFirstName, author.name = lastName as isLastName\n",
    "MERGE (a)-[:AUTHORED {is_first_author:isFirstName, is_last_author:isLastName}]-(i)\n",
    "RETURN *\n",
    "\"\"\"\n",
    "\n",
    "graph.run(import_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test citation relationships\n",
    "query1 = \"\"\"\n",
    "Match (a:Quanta {title: \"Paper13\"})\n",
    "Match (b:Quanta {title: \"Paper7\"})\n",
    "WITH a, b\n",
    "MERGE (a)-[:CITES]->(b)\"\"\"\n",
    "graph.run(query1)\n",
    "\n",
    "query2 = \"\"\"\n",
    "Match (c:Quanta {title: \"Paper13\"})\n",
    "Match (d:Quanta {title: \"Paper1\"})\n",
    "WITH c, d\n",
    "MERGE (c)-[:CITES]->(d)\n",
    "\"\"\"\n",
    "graph.run(query2)\n",
    "\n",
    "query3 = \"\"\"\n",
    "Match (e:Quanta {title: \"Paper7\"})\n",
    "Match (f:Quanta {title: \"Paper1\"})\n",
    "WITH e, f\n",
    "MERGE (e)-[:CITES]->(f)\n",
    "\"\"\"\n",
    "graph.run(query3)\n",
    "\n",
    "query4 = \"\"\"\n",
    "Match (g:Quanta {title: \"Paper13\"})\n",
    "Match (h:Quanta {title: \"Paper8\"})\n",
    "WITH g, h\n",
    "MERGE (g)-[:CITES]->(h)\n",
    "\"\"\"\n",
    "graph.run(query4)\n",
    "\n",
    "query5 = \"\"\"\n",
    "Match (i:Quanta {title: \"Paper7\"})\n",
    "Match (j:Quanta {title: \"Paper8\"})\n",
    "WITH i, j\n",
    "MERGE (i)-[:CITES]->(j)\n",
    "\"\"\"\n",
    "graph.run(query5)\n",
    "\n",
    "query6 = \"\"\"\n",
    "Match (k:Quanta {title: \"Paper1\"})\n",
    "Match (l:Quanta {title: \"Paper8\"})\n",
    "WITH k, l\n",
    "MERGE (k)-[:CITES]->(l)\n",
    "\n",
    "\"\"\"\n",
    "graph.run(query6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_extra_query = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE size((n)--())=0\n",
    "DELETE (n)\"\"\"\n",
    "graph.run(delete_extra_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastest: Put more work on thread running in parallel. \n",
    "print(\"Adding citation relationships...\", end=\" \", flush=True)\n",
    "# query = \"\"\"\n",
    "# MATCH (b:Quanta) UNWIND b.refs AS ref\n",
    "# MATCH (a:Quanta {id: ref}) MERGE (b)-[:CITES]->(a);\n",
    "# \"\"\"\n",
    "\n",
    "Note this query ^^ should include the apoc.periodic.iterate() function, which would be:\n",
    "query = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "\"MATCH (b:Quanta) UNWIND b.refs AS ref RETURN b, ref\",\n",
    "\"MATCH (a:Quanta {id: ref}) MERGE (b)-[:CITES]->(a)\",\n",
    "{batchSize:500, iterateList:true, parallel:false});\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = graph.database.primitive_counts['NumberOfNodeIdsInUse']\n",
    "n_relationships = graph.database.primitive_counts['NumberOfRelationshipIdsInUse']\n",
    "print(\"Created graph database with {:,} nodes and {:,} relationships!\".format\n",
    "     (n_nodes, n_relationships))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various misc scripts below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PageRank on each year from 1800 to 1805\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "start_year, end_year = 1985, 2010\n",
    "dfs = []\n",
    "for year in range(start_year, end_year+1):\n",
    "\n",
    "    print(\"Running PageRank on works from <= {}...\".format(year), end=\" \")\n",
    "    query = \"\"\"\n",
    "    CALL algo.pageRank(\n",
    "    'MATCH (p:Quanta) WHERE p.year <= {} RETURN id(p) as id',\n",
    "    'MATCH (p1:Quanta)-[:CITES]->(p2:Quanta) RETURN id(p1) as source, id(p2) as target',\n",
    "    {{graph:'cypher', writeProperty:'pageRank_{}', iterations:5, write: true, concurrency:20}});\n",
    "    \"\"\".format(year,year)\n",
    "    graph.run(query).evaluate()\n",
    "    \n",
    "    print(\"Pulling out and saving results...\", end=\" \")\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Quanta) \n",
    "    WHERE a.year <= {} \n",
    "    RETURN id(a), a.title, a.pageRank_{}\"\"\".format(year,year)\n",
    "    df = graph.run(query).to_data_frame()\n",
    "    df['year'] = year\n",
    "    dfs.append(df)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "result = pd.concat(dfs).pivot_table(index='a.title', columns='year', values='a.pageRank')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(dfs).pivot_table(index='a.title', columns='year', values='a.pageRank')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write result to CSV\n",
    "file_path = '/tmp/data/result/impact_20M_{}-{}.csv'.format(start_year, end_year)\n",
    "print(\"Writing results to {}...\".format(file_path), end=\" \")\n",
    "result.index = result.index.str.replace(\",\",\"\")\n",
    "result.to_csv(path_or_buf=file_path, sep=\",\", header=True, index=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "scores = result.sum(axis=1)\n",
    "scores.plot.hist(grid=True,bins=[i/2 for i in range(1,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = scores.quantile(0.999999)\n",
    "top_papers = scores.drop(scores[scores.values>=15].index)\n",
    "print(\"Considering the top {} (score >= {:.2f}) papers.\".format(len(top_papers), score_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_lists = top_papers.index.to_series().apply(\n",
    "    lambda x: [w for w in re.compile(r\"[A-Za-z']{4,}\").findall(x)])\n",
    "all_words = set()\n",
    "word_map = {}\n",
    "for i,v in word_lists.items():\n",
    "    for w in v:\n",
    "        all_words.add(w.lower())\n",
    "        word_map.get(w,[]).append(i)\n",
    "print(\"Built set and map of {:,} unique words.\".format(len(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filling {}x{} dataframe...\".format(len(all_words),len(result.columns)), end = \"\")\n",
    "word_scores = pd.DataFrame(0, index=all_words, columns=result.columns)\n",
    "for key, value in word_map:\n",
    "    print(key)\n",
    "#     for w in ws:\n",
    "#         print(w)\n",
    "#         word_scores.loc[w.lower()] += result.loc[title].va\n",
    "#         print(word_scores.loc[w.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[title,:]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
