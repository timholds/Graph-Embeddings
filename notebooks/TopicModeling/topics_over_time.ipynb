{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "# Need to get authentication working, currently NEO4J_AUTH=none\n",
    "graph = Graph(\"bolt://neo4j:7687\")\n",
    "# graph = Graph('bolt://localhost:7687', bolt=True)\n",
    "\n",
    "n_nodes = graph.database.primitive_counts['NumberOfNodeIdsInUse']\n",
    "n_relationships = graph.database.primitive_counts['NumberOfRelationshipIdsInUse']\n",
    "print(\"Connected to graph database with {:,} nodes and {:,} relationships!\".format\n",
    "     (n_nodes, n_relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write queries to CSV files\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "query = \"\"\"CALL apoc.export.csv.query(\"MATCH (q:Quanta) WHERE (q.venue=\\\\\"Nature\\\\\" OR q.venue=\\\\\"Science\\\\\") AND q.year>=1990 AND EXISTS(q.fos) RETURN q.title as title, q.venue as venue, q.fos as fos, q.year as year\", \"/import/result/AllQuantaInNatureScienceYear1990.csv\", {})\"\"\"\n",
    "graph.run(querviy).evaluate()\n",
    "print(query)\n",
    "print(\"Finished query and wrote results in {:.2f} seconds.\".format(time.time()-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "query = \"\"\"CALL apoc.export.csv.query(\"MATCH (q:Quanta) WHERE ( q.venue=\\\\\"CA: A Cancer Journal for Clinicians\\\\\" OR q.venue=\\\\\"The New England Journal of Medicine\\\\\" OR q.venue=\\\\\"The Lancet\\\\\" OR q.venue=\\\\\"Chemical Reviews\\\\\" OR q.venue=\\\\\"Nature Reviews Drug Discovery\\\\\" OR q.venue=\\\\\"JAMA\\\\\" OR q.venue=\\\\\"Nature Reviews Cancer\\\\\" OR q.venue=\\\\\"Nature Reviews Immunology\\\\\" OR q.venue=\\\\\"Nature\\\\\" OR q.venue=\\\\\"Nature Reviews Genetics\\\\\" OR q.venue=\\\\\"Science\\\\\" OR q.venue=\\\\\"Chemical Society Reviews\\\\\" OR q.venue=\\\\\"Nature Materials\\\\\" OR q.venue=\\\\\"Nature Nanotechnology\\\\\" OR q.venue=\\\\\"Lancet Oncology\\\\\" OR q.venue=\\\\\"Reviews of Modern Physics\\\\\" OR q.venue=\\\\\"Nature Biotechnology\\\\\" OR q.venue=\\\\\"Nature Reviews Molecular Cell Biology\\\\\" OR q.venue=\\\\\"Nature Reviews Neuroscience\\\\\" OR q.venue=\\\\\"Nature Medicine\\\\\" OR q.venue=\\\\\"Nature Photonics\\\\\" OR q.venue=\\\\\"Nature Reviews Microbiology\\\\\" OR q.venue=\\\\\"Cell\\\\\" OR q.venue=\\\\\"Advances in Physics\\\\\" OR q.venue=\\\\\"Energy and Environmental Science\\\\\" OR q.venue=\\\\\"World Psychiatry\\\\\" ) AND q.year>=1990 AND EXISTS(q.fos) RETURN q.title as title, q.venue as venue, q.fos as fos, q.year as year\", \"/import/result/AllQuantaWithIf30Year1990.csv.csv\", {})\"\"\"\n",
    "print(query)\n",
    "# graph.run(query).evaluate()\n",
    "print(\"Finished query and wrote results in {:.2f} seconds.\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ArticleRank\n",
    "year = 2018\n",
    "print(\"Running ArticleRank on works from <= {}...\".format(year), end=\" \")\n",
    "query = \"\"\"\n",
    "CALL algo.articleRank(\n",
    "'MATCH (p:Quanta) WHERE p.year <= {} RETURN id(p) as id',\n",
    "'MATCH (p1:Quanta)-[:CITES]->(p2:Quanta) RETURN id(p1) as source, id(p2) as target',\n",
    "{{graph:'cypher', writeProperty:'articleRank{}', write: true}});\n",
    "\"\"\".format(year,year)\n",
    "print(query)\n",
    "# graph.run(query).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries for topic modeling\n",
    "import os\n",
    "import gensim\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query results as list of lists\n",
    "# df = pd.read_csv('/tmp/data/result/AllQuantaInNatureScienceYear1990.csv')\n",
    "df = pd.read_csv('/tmp/data/result/AllQuantaWithIf30Year1990.csv.csv')\n",
    "\n",
    "# Process\"Field of Study\" field\n",
    "df['fos'] = df['fos'].apply(eval)\n",
    "train_text_fos = df['fos'].tolist()\n",
    "\n",
    "# Process \"title\" field\n",
    "def tokenize_title(title):\n",
    "    title = remove_stopwords(title.lower())\n",
    "    return gensim.utils.simple_preprocess(title, deacc=True)\n",
    "train_text_title = df['title'].apply(tokenize_title).tolist()\n",
    "\n",
    "# Merge tokens from \"title\" and \"fos\"\n",
    "for i in range(len(train_text_fos)):\n",
    "    train_text_fos[i].extend(train_text_title[i])\n",
    "train_text = train_text_fos\n",
    "\n",
    "# Capitalize everything\n",
    "train_text = [[w.upper() for w in line] for line in train_text]\n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train corpus from query result\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "train_dictionary = Dictionary(train_text)\n",
    "train_corpus = [train_dictionary.doc2bow(t) for t in train_text]\n",
    "print(\"Training data with {:,} samples created.\".format(len(train_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "start_time = time.time()\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=train_corpus, \n",
    "    num_topics=400,\n",
    "    id2word=train_dictionary,\n",
    "    chunksize=10000\n",
    "    )\n",
    "\n",
    "print(\"Trained NDA model with {} topics in {:.2f} minutes.\".format(lda_model.num_topics, (time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top topics\n",
    "lda_model.print_topics(num_topics=400, num_words=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LDA model a bit\n",
    "\n",
    "# Find the topics most relevant to some words of interest\n",
    "word_id = train_dictionary.token2id[\"SYNTHETIC\"]\n",
    "print(lda_model.get_term_topics(word_id, minimum_probability=None))\n",
    "\n",
    "# Make sure predictions are working as expected\n",
    "row = df.loc[1,]\n",
    "text = [w.upper() for w in row['fos']]\n",
    "mapped_text = train_dictionary.doc2bow(text)\n",
    "topic_weights = lda_model[mapped_text]\n",
    "topic_words = [train_dictionary[w[0]] for w in topic_weights]\n",
    "print(topic_weights)\n",
    "print(topic_words)\n",
    "# test_df = df.iloc[1:10,:]\n",
    "# test_df.head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "def calculate_topics(token_list):\n",
    "    text = [w.upper() for w in token_list]\n",
    "    mapped_text = train_dictionary.doc2bow(text)\n",
    "    topic_weights = lda_model[mapped_text]\n",
    "    topic_words = [train_dictionary[w[0]] for w in topic_weights]\n",
    "    return [topic_weights, topic_words]\n",
    "\n",
    "topic_weights_and_words = df['fos'].progress_apply(calculate_topics)\n",
    "df['topic_weight_list'] = topic_weights_and_words.apply(lambda x: x[0])\n",
    "df['topic_words'] = topic_weights_and_words.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_frame_list(df, target_column):\n",
    "    row_accumulator = []\n",
    "    def split_list_to_rows(row):\n",
    "        split_row = row[target_column]\n",
    "        if isinstance(split_row, list):\n",
    "            for s in split_row:\n",
    "                new_row = row.to_dict()\n",
    "                new_row[target_column] = s\n",
    "                row_accumulator.append(new_row)\n",
    "        else:\n",
    "            print(\"ERROR!\")\n",
    "    df.progress_apply(split_list_to_rows, axis=1)\n",
    "    new_df = pd.DataFrame(row_accumulator)\n",
    "    return new_df\n",
    "\n",
    "# Split each row into multiple rows (one for each topic/weight pair)\n",
    "df_compiled = split_data_frame_list(df,'topic_weight_list')\n",
    "\n",
    "# Concatenate topic/weight columns with full dataframe\n",
    "df_compiled[['topic_id', 'topic_weight']] = pd.DataFrame(df_compiled['topic_weight_list'].values.tolist())\n",
    "\n",
    "# Drop now unnecessary topic/weight column\n",
    "df_compiled = df_compiled.drop('topic_weight_list', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df_compiled[df_compiled['topic_weight'] != 0]\n",
    "df_sum['topic_words_str'] = df_sum['topic_words'].apply(lambda x: )\n",
    "\n",
    "print(\"Max: {:.2f}\".format(df_sum['topic_weight'].max()))\n",
    "print(\"Min: {:.2f}\".format(df_sum['topic_weight'].min()))\n",
    "print(\"Average: {:.2f}\".format(df_sum['topic_weight'].mean()))\n",
    "print(\"Median: {:.2f}\".format(df_sum['topic_weight'].median()))\n",
    "print(\"Most frequent value: {:.2f}\".format(df_sum['topic_weight'].round(3).value_counts().idxmax()))\n",
    "\n",
    "df_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = df_sum['topic_id']==236\n",
    "mask2 = df_sum['topic_id']==386\n",
    "mask3 = df_sum['topic_id']==383\n",
    "mask4 = df_sum['topic_id']==389\n",
    "\n",
    "p = sns.factorplot(x=\"year\", y='topic_weight', col='topic_id', col_wrap=2, \n",
    "                   kind='strip', jitter=1, \n",
    "                   data=df_sum[mask1.values | mask2.values | mask3.values | mask4.values])\n",
    "p.fig.subplots_adjust(top=0.85)\n",
    "p.fig.suptitle(\"Scatterplot of Normalized Topic Weights, Split by Topic; All Weights.\", fontsize=12)\n",
    "p.set_xticklabels(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_map = {236: 'CRISPR, PEPTIDES, CAS',\n",
    "            386: 'MRNA, MESSENGER RNA, BIOINFORMATICS',\n",
    "            11:  'STEM, BIOLOGY, CELL',\n",
    "            382: 'CANCER, ONCOLOGY, MEDICINE',\n",
    "            18:  'GENETICS, RNA INTERFERENCE, MOLECULAR BIOLOGY'}\n",
    "\n",
    "mask = (df_sum['topic_id']==236) | (df_sum['topic_id']==386) | (df_sum['topic_id']==11) |(df_sum['topic_id']==382) | (df_sum['topic_id']==18)\n",
    "df_plot = df_sum[mask]\n",
    "df_plot['Topic'] = df_plot['topic_id'].map(topic_map)\n",
    "\n",
    "sns.set(style=\"ticks\", rc={\"lines.linewidth\": 1})\n",
    "p = sns.factorplot(x=\"year\", y='topic_weight', kind='point', hue='Topic', linestyles=\":\",\n",
    "                   size=8, aspect=1.5, data=df_plot)\n",
    "p.set_xticklabels(rotation=90)\n",
    "p.set_xlabels(\"Year\")\n",
    "p.set_ylabels(\"Topic Weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot['scaled_weight'] = df_plot['topic_weight']\n",
    "for t in df_plot['topic_id'].unique():\n",
    "    print(\"hi, {}\".format(t))\n",
    "    t_max = df_plot[df_plot['topic_id']==t]['topic_weight'].max()\n",
    "    topic_scaled = df_plot[df_plot['topic_id']==t]['topic_weight']/t_max\n",
    "    df_plot['scaled_weight'][df_plot['topic_id']==t] = topic_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot[df_plot['topic_id']==386]['scaled_weight'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.factorplot(x=\"year\", y='scaled_weight', kind='point', hue='Topic', \n",
    "                   linestyles=\":\", size=8, aspect=1.5, data=df_plot)\n",
    "p.set_xticklabels(rotation=90)\n",
    "p.set_xlabels(\"Year\")\n",
    "p.set_ylabels(\"Topic Weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_map = {}\n",
    "for t in tqdm_notebook(df_sum['topic_id'].unique()):\n",
    "    topic_terms[t] = \", \".join([w[0] for w in lda_model.show_topic(t, topn=3)])\n",
    "    \n",
    "df_sum['topic'] = df_sum['topic_id'].map(topic_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 400 topics to look for the next CRISPR\n",
    "# step_size = 400\n",
    "# for i in range(step_size,df_sum['topic_id'].max(), step_size):\n",
    "    subsssub\n",
    "    \n",
    "#     df_plot_all = df_sum[(df_sum['topic_id']>=(i-step_size)) & (df_sum['topic_id']<i)]\n",
    "    \n",
    "p = sns.factorplot(x=\"year\", y='topic_weight', kind='point', col='topic_id', \n",
    "                   linestyles=\":\", size=2, aspect=2, col_wrap=4, \n",
    "                   data=df_sum)\n",
    "p.set_xticklabels(rotation=90)\n",
    "p.set_xlabels(\"Year\")\n",
    "p.set_ylabels(\"Topic Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot interesting topics in more detail\n",
    "mask = df_sum['topic_id']==46\n",
    "# df_sum[mask]['title'].apply(print)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context('paper')\n",
    "ax = sns.lineplot(x='year', y='topic_weight', data=df_sum[mask])\n",
    "ax.set(xlabel=\"Year\", ylabel=\"Topic Weight\", title=\"ECOLOGY, DEPENDENT, PEPTIDE, BIOLOGICAL\")\n",
    "ax\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot interesting topics in more detail\n",
    "mask = (df_sum['topic_id']==46)|(df_sum['topic_id']==65)|(df_sum['topic_id']==337)\n",
    "# df_sum[mask]['title'].apply(print)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context('paper')\n",
    "ax = sns.lineplot(x='year', y='topic_weight', hue='topic', style='topic', data=df_sum[mask], legend=False)\n",
    "ax.set(xlabel=\"Year\", ylabel=\"Topic Weight\")\n",
    "plt.legend([\"ECOLOGY, DEPENDENT, PEPTIDE\",\"LIVING, AUTISM, HIGHER\",\"PHYSICS, NANOTECHNOLOGY, CHEMISTRY\"],loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
