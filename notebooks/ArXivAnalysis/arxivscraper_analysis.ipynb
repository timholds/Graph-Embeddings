{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data analysis & viz libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import nlp libraries\n",
    "import string\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# set nlp variables\n",
    "english_stops = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nlp functions\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    \"\"\" Lowercases, takes out punct and stopwords and short strings \"\"\"\n",
    "    return [token.lower() for token in tokens if (token not in string.punctuation) and \n",
    "                   (token.lower() not in english_stops) and len(token) > 2]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    \"\"\" Removes plurals \"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def count_ngrams(tokens,n):\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    ngram_freq = collections.Counter(n_grams)\n",
    "    ngram_freq = ngram_freq.most_common()\n",
    "    return ngram_freq\n",
    "\n",
    "def ngram_to_dict(ngram_freq):\n",
    "    l = []\n",
    "    for t in ngram_freq:\n",
    "        l.append((' '.join(t[0]),t[1]))\n",
    "    return dict(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle\n",
    "cat = 'physics'\n",
    "articles = pd.read_pickle('{}_through_2018-11-18'.format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge & drop duplicates\n",
    "articles = articles.copy()\n",
    "articles.drop_duplicates('id',inplace=True)\n",
    "\n",
    "# add month & year column\n",
    "articles['year'] = pd.to_datetime(articles['year-month'],format='%Y-%m').dt.strftime('%Y')\n",
    "\n",
    "# put a space at the end of each abstract\n",
    "articles['abstract+'] = articles['abstract'].apply(lambda abs: abs+' ')\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group abstracts by month and year\n",
    "articles_year = articles.groupby('year')['abstract+'].agg(['count','sum'])\n",
    "articles_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "bigram_dict = {}\n",
    "trigram_dict = {}\n",
    "ngram_dict = {}\n",
    "\n",
    "for year in articles_year.index:\n",
    "    tokens = nltk.word_tokenize(articles_year.loc[year,'sum'])\n",
    "    wc = len(tokens)\n",
    "    articles_year.loc[year,\"word count\"] = wc\n",
    "    clean = clean_tokens(tokens)\n",
    "    lem = lemmatize(clean)\n",
    "    \n",
    "    # count word and ngram frequency\n",
    "    word_freq = count_ngrams(lem, 1)\n",
    "    bigram_freq = count_ngrams(lem, 2)\n",
    "    trigram_freq = count_ngrams(lem, 3)\n",
    "    ngram_freq = word_freq + bigram_freq + trigram_freq\n",
    "    \n",
    "    # change to dict\n",
    "    word_freq = ngram_to_dict(word_freq)\n",
    "    bigram_freq = ngram_to_dict(bigram_freq)\n",
    "    trigram_freq = ngram_to_dict(trigram_freq)\n",
    "    ngram_freq = ngram_to_dict(ngram_freq)\n",
    "    \n",
    "    # add year metadata\n",
    "    word_dict[year] = word_freq\n",
    "    bigram_dict[year] = bigram_freq\n",
    "    trigram_dict[year] = trigram_freq\n",
    "    ngram_dict[year] = ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle ai & ai_year with word count\n",
    "articles.to_pickle('{}_through_2018-12-08'.format(cat))\n",
    "articles_year.to_pickle('{}_only_year_through_2018-12-08'.format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn into dfs\n",
    "def dict_to_df(dictionary, year):\n",
    "    df = pd.DataFrame(dictionary)\n",
    "    df.sort_values(year,ascending=False, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "word_df = dict_to_df(word_dict,'2018')\n",
    "bigram_df = dict_to_df(bigram_dict,'2018')\n",
    "trigram_df = dict_to_df(trigram_dict,'2018')\n",
    "\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as pickle files\n",
    "word_df.to_pickle('{}_words_df'.format(cat))\n",
    "bigram_df.to_pickle('{}_bigrams_df'.format(cat))\n",
    "trigram_df.to_pickle('{}_trigrams_df'.format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as csv files\n",
    "word_df.to_csv('{}_words_df.csv'.format(cat))\n",
    "bigram_df.to_csv('{}_bigrams_df.csv'.format(cat))\n",
    "trigram_df.to_csv('{}_trigrams_df.csv'.format(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from pickle files\n",
    "articles_year = pd.read_pickle('{}_only_year_through_2018-12-08'.format(cat))\n",
    "\n",
    "word_df = pd.read_pickle('{}_words_df'.format(cat))\n",
    "bigram_df = pd.read_pickle('{}_bigrams_df'.format(cat))\n",
    "trigram_df = pd.read_pickle('{}_trigrams_df'.format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frequency per 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_pre_1998(df):\n",
    "    for year in range(1993,1999):\n",
    "        df.drop(str(year),axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_freq(df,n):\n",
    "    freq_per_n = df.copy()\n",
    "    freq_per_n.fillna(0,inplace=True)\n",
    "\n",
    "    for year in freq_per_n.columns:\n",
    "        wc = articles_year.loc[year,'word count']\n",
    "        wc_normalized = wc/n\n",
    "        freq_per_n[year] = freq_per_n[year]/wc_normalized\n",
    "    \n",
    "    freq_per_n.reset_index(inplace=True)\n",
    "    \n",
    "    return freq_per_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_per_1000 = drop_pre_1998(calculate_freq(word_df, 1000))\n",
    "bigram_freq_per_1000 = drop_pre_1998(calculate_freq(bigram_df, 1000))\n",
    "trigram_freq_per_1000 = drop_pre_1998(calculate_freq(trigram_df, 1000))\n",
    "\n",
    "word_freq_per_1000.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_freq(freq_df):\n",
    "    avg_freq = freq_df.copy()\n",
    "    avg_freq.set_index('index',inplace=True)\n",
    "    \n",
    "    # calculate average frequency\n",
    "    avg_freq['avg freq'] = avg_freq.mean(axis=1)\n",
    "    \n",
    "    # keep only top 100\n",
    "    avg_freq.sort_values('avg freq',ascending=False,inplace=True)\n",
    "    top_freq = avg_freq.iloc[:100]\n",
    "    \n",
    "    return top_freq\n",
    "\n",
    "def calculate_gains(freq_df):\n",
    "    gains = freq_df.copy()\n",
    "    gains['% gain'] = (gains['2018']-gains['1999'])/gains['1999']\n",
    "    \n",
    "    gains.sort_values(['% gain','avg freq'],ascending=[False,False],inplace=True)\n",
    "    \n",
    "    return gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_five_yr_gains(freq_df):\n",
    "    gains = freq_df.copy()\n",
    "    gains['% gain'] = (gains['2014-2018']-gains['1999-2003'])/gains['1999-2003']\n",
    "    \n",
    "    gains.sort_values(['% gain','avg freq'],ascending=[False,False],inplace=True)\n",
    "    \n",
    "    return gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_gains = calculate_five_yr_gains(top_freq(calculate_five_year(word_freq_per_1000)))\n",
    "bigram_gains = calculate_five_yr_gains(top_freq(calculate_five_year(bigram_freq_per_1000)))\n",
    "trigram_gains = calculate_five_yr_gains(top_freq(calculate_five_year(trigram_freq_per_1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_gains.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_gains.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_gains.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_gains.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_gains.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_gains.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best(best_of,df):\n",
    "    best_of_df = df[df['index'].apply(lambda ngram: ngram in best_of)]\n",
    "    best_of_df = df[df['index'].apply(lambda ngram: ngram in best_of)]\n",
    "    best_of_df = best_of_df.set_index('index')\n",
    "    \n",
    "    return best_of_df\n",
    "\n",
    "def plot_best(best_of_df):\n",
    "    best_of_df = best_of_df.T\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(best_of_df.index,best_of_df)\n",
    "    plt.legend(best_of_df.columns)\n",
    "    sns.despine(left=True,bottom=True)\n",
    "\n",
    "# def plot_heatmap(best_of_df):\n",
    "#     ax = sns.heatmap(best_of_df)\n",
    "\n",
    "def calculate_five_year(freq_df):\n",
    "    five_year = pd.DataFrame()\n",
    "    five_year['index'] = freq_df['index']\n",
    "    five_year['1999-2003'] = freq_df.loc[:,[str(year) for year in range(1999,2004)]].mean(axis=1)\n",
    "    five_year['2004-2008'] = freq_df.loc[:,[str(year) for year in range(2004,2009)]].mean(axis=1)\n",
    "    five_year['2009-2013'] = freq_df.loc[:,[str(year) for year in range(2009,2014)]].mean(axis=1)\n",
    "    five_year['2014-2018'] = freq_df.loc[:,[str(year) for year in range(2014,2019)]].mean(axis=1)\n",
    "    \n",
    "    return five_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gains cleaned\n",
    "words_up = word_gains.head(6).index\n",
    "words_down = word_gains.tail(6).index\n",
    "\n",
    "plot_best(select_best(words_up,calculate_five_year(word_freq_per_1000)))\n",
    "plot_best(select_best(words_down,calculate_five_year(word_freq_per_1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "\n",
    "gains = select_best(words_up,word_freq_per_1000).iloc[:,1:]\n",
    "losses = select_best(words_down,word_freq_per_1000).iloc[:,1:]\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,6),sharex=True)\n",
    "cbar_ax = fig.add_axes([.94, .3, .015, .4])\n",
    "sns.heatmap(gains,ax=ax[0],vmin=0,vmax=8,cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "sns.heatmap(losses,ax=ax[1],vmin=0,vmax=8,cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "\n",
    "#remove y label\n",
    "ax[0].set_ylabel('')\n",
    "ax[1].set_ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power normalized\n",
    "from matplotlib.colors import PowerNorm\n",
    "\n",
    "sns.set(style='white')\n",
    "\n",
    "# normalize colorbar\n",
    "power_norm = PowerNorm(gamma=0.65)\n",
    "\n",
    "# plot\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,6),sharex=True)\n",
    "cbar_ax = fig.add_axes([.94, .3, .015, .4])\n",
    "hm1 = sns.heatmap(gains,ax=ax[0],vmin=0,vmax=10,norm=power_norm,cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "hm2 = sns.heatmap(losses,ax=ax[1],vmin=0,vmax=10,norm=power_norm,cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "# fig.colorbar(hm1,ax=cbar_ax, extend='max')\n",
    "\n",
    "# remove y label\n",
    "ax[0].set_ylabel('')\n",
    "ax[1].set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams cleaned\n",
    "bigrams_up = bigram_gains.head(6).index\n",
    "bigrams_down = bigram_gains.tail(6).index\n",
    "\n",
    "bigram_five_year = calculate_five_year(bigram_freq_per_1000)\n",
    "bigram_five_year.set_index('index',inplace=True)\n",
    "bigram_five_year.reset_index(inplace=True)\n",
    "\n",
    "plot_best(select_best(bigrams_up,bigram_five_year))\n",
    "plot_best(select_best(bigrams_down,bigram_five_year))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "\n",
    "bigram_freq_tweaked = bigram_freq_per_1000.copy()\n",
    "bigram_freq_tweaked.set_index('index',inplace=True)\n",
    "bigram_freq_tweaked.reset_index(inplace=True)\n",
    "\n",
    "gains = select_best(bigrams_up,bigram_freq_tweaked).iloc[:,1:]\n",
    "losses = select_best(bigrams_down,bigram_freq_tweaked).iloc[:,1:]\n",
    "\n",
    "# normalize colorbar\n",
    "power_norm = PowerNorm(gamma=0.5)\n",
    "\n",
    "# plot\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,6),sharex=True)\n",
    "cbar_ax = fig.add_axes([.94, .3, .015, .4])\n",
    "sns.heatmap(gains,ax=ax[0],vmin=0,vmax=2,norm=power_norm, cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "sns.heatmap(losses,ax=ax[1],vmin=0,vmax=2,norm=power_norm, cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "\n",
    "#remove y label\n",
    "ax[0].set_ylabel('')\n",
    "ax[1].set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams cleaned\n",
    "trigram_up = trigram_gains.head(6).index\n",
    "trigram_down = trigram_gains.tail(6).index\n",
    "\n",
    "trigram_five_year = calculate_five_year(trigram_freq_per_1000)\n",
    "trigram_five_year.set_index('index',inplace=True)\n",
    "trigram_five_year.reset_index(inplace=True)\n",
    "\n",
    "plot_best(select_best(trigram_up,trigram_five_year))\n",
    "plot_best(select_best(trigram_down,trigram_five_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "\n",
    "trigram_freq_tweaked = trigram_freq_per_1000.copy()\n",
    "trigram_freq_tweaked.set_index('index',inplace=True)\n",
    "trigram_freq_tweaked.reset_index(inplace=True)\n",
    "\n",
    "gains = select_best(trigrams_up,trigram_freq_tweaked).iloc[:,1:]\n",
    "losses = select_best(trigrams_down,trigram_freq_tweaked).iloc[:,1:]\n",
    "\n",
    "# normalize colorbar\n",
    "power_norm = PowerNorm(gamma=0.5)\n",
    "\n",
    "# plot\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,6),sharex=True)\n",
    "cbar_ax = fig.add_axes([.94, .3, .015, .4])\n",
    "sns.heatmap(gains,ax=ax[0],vmin=0,vmax=2,norm=power_norm, cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "sns.heatmap(losses,ax=ax[1],vmin=0,vmax=2,norm=power_norm, cbar_ax=cbar_ax,cmap=\"Reds\")\n",
    "\n",
    "#remove y label\n",
    "ax[0].set_ylabel('')\n",
    "ax[1].set_ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
