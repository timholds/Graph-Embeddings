{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from py2neo import Graph, Node, Relationship\n",
    "#from py2neo.Graph import database \n",
    "\n",
    "# Need to get authentication working, currently NEO4J_AUTH=none\n",
    "graph = Graph(\"bolt://neo4j:7687\")\n",
    "# graph = Graph('bolt://localhost:7687', bolt=True)\n",
    "\n",
    "#graph.delete_all()\n",
    "\n",
    "n_nodes = graph.database.primitive_counts['NumberOfNodeIdsInUse']\n",
    "n_relationships = graph.database.primitive_counts['NumberOfRelationshipIdsInUse']\n",
    "print(\"Connected to graph database with {:,} nodes and {:,} relationships!\".format\n",
    "     (n_nodes, n_relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import the publications where lang = 'en' and publisher = \"Science\" or \"Nature\" in year 2008\n",
    "import pandas as pd\n",
    "import time\n",
    "print(\"load english science and nature publication into df\")\n",
    "start_time = time.time()\n",
    "query = \"\"\"\n",
    "MATCH (n:Quanta) WHERE n.lang = 'en' AND ( n.venue = 'Science' OR n.venue = 'Nature') AND n.year =2008 AND EXISTS(n.fos)\n",
    "RETURN \n",
    "n.venue as venue,\n",
    "n.pageRank_2018 as PR_2018,\n",
    "n.pageRank_2008 as PR_2008,\n",
    "n.fos as fos,\n",
    "n.title as title,\n",
    "n.keywords as keywords,\n",
    "n.publisher as publisher\n",
    "ORDER BY n.pageRank_2018 DESC\n",
    "\"\"\"\n",
    "#n.keywords as keywords\n",
    "dfs_2008_test = graph.run(query).to_data_frame()\n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))\n",
    "#dfs_2008_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_range = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import itertools\n",
    "\n",
    "dfs_2008_test_copy = dfs_2008_test.copy()\n",
    "start_time = time.time()\n",
    "fos_list = dfs_2008_test_copy[\"fos\"].tolist()\n",
    "fos_list = [[] if v is None else v for v in fos_list]\n",
    "\n",
    "\n",
    "dfs_2008_test_copy.head()\n",
    "#Replace original fos with updated fos \n",
    "dfs_2008_test_copy['fos'] = pd.Series(fos_list).values\n",
    "dfs_2008_test\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "X = mlb.fit_transform(dfs_2008_test_copy.fos)\n",
    "dfs_2008_test_copy = dfs_2008_test_copy.join(pd.DataFrame(X, columns=mlb.classes_))\n",
    "\n",
    "#del fos in the df\n",
    "\n",
    "del dfs_2008_test_copy['fos']\n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))\n",
    "dfs_2008_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(dfs_2008_test.title).toarray()\n",
    "#labels = df.category_id\n",
    "print(features.shape)\n",
    "\n",
    "print(len(tfidf.get_feature_names()))\n",
    "print(type(features))\n",
    "\n",
    "title_feature_name = tfidf.get_feature_names()\n",
    "count_featurename = len(title_feature_name)\n",
    "for i in range(count_featurename):\n",
    "    column_name = 'title_'+title_feature_name[i]\n",
    "    dfs_2008_test_copy[column_name] = pd.Series(features[:,i]).values\n",
    "\n",
    "dfs_2008_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDummies = pd.get_dummies(dfs_2008_test_copy['venue'], prefix = 'venue')\n",
    "dfs_2008_test_copy = pd.concat([dfs_2008_test_copy, dfDummies], axis=1)\n",
    "del dfs_2008_test_copy['venue']\n",
    "del dfs_2008_test_copy['keywords']\n",
    "del dfs_2008_test_copy['publisher']\n",
    "del dfs_2008_test_copy['PR_2008']\n",
    "del dfs_2008_test_copy['title']\n",
    "del dfs_2008_test_copy['PR_2018']\n",
    "dfs_2008_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import (metrics, cross_validation, linear_model, preprocessing)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "\n",
    "def auc_machine(df, top_pt, algo):\n",
    "    \n",
    "    #add the result into dataframe\n",
    "    if 'popular' in df:\n",
    "        del df['popular']\n",
    "    \n",
    "    row_n = len(df)\n",
    "    index = int(row_n*top_pt)\n",
    "    \n",
    "    popular_result = []\n",
    "    for i in range(row_n):\n",
    "        if i < index:\n",
    "            popular_result.append(1)\n",
    "        else:\n",
    "            popular_result.append(0)\n",
    "    \n",
    "    df['popular'] = pd.Series(popular_result).values\n",
    "    \n",
    "    #make a balanced dataframe\n",
    "    df_high = pd.DataFrame() \n",
    "    df_low = pd.DataFrame()\n",
    "    df_new = pd.DataFrame()\n",
    "    \n",
    "    if top_pt <= 0.5:\n",
    "        df_high = df.iloc[:index,:].copy()\n",
    "        df_temp = df.iloc[index:,:].copy()\n",
    "        n_row_less = len(df_high)\n",
    "        df_low = df_temp.sample(n_row_less)\n",
    "        df_new = pd.concat([df_high, df_low], ignore_index=True)\n",
    "    \n",
    "#     elif top_pt == 0.5:\n",
    "#         df_new = df.copy()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        df_temp = df.iloc[:index,:].copy()\n",
    "        df_low = df.iloc[index:,:].copy()\n",
    "        n_row_less = len(df_low)\n",
    "        df_high = df_temp.sample(n_row_less)\n",
    "    df_new = pd.concat([df_low, df_high], ignore_index=True)\n",
    "\n",
    "    \n",
    "    if (algo == 'rf' or algo == 'lg'):\n",
    "        #finished setting up models \n",
    "        feature_list = list(df.columns.values)\n",
    "        df_X = df_new[feature_list[:-1]]\n",
    "        df_y = df_new['popular']\n",
    "    #     df_X = df[feature_list[:-1]]\n",
    "    #     df_y = df['popular']\n",
    "\n",
    "        #build up a new imbalanced \n",
    "        if algo == 'rf':\n",
    "            clf = RandomForestClassifier(min_samples_split =7, n_estimators=100)\n",
    "        elif algo == 'lg':\n",
    "            clf = LogisticRegression()\n",
    "        mean_auc = []\n",
    "\n",
    "        SEED=43\n",
    "        n = 4  # repeat the CV procedure 10 times to get more precise results\n",
    "        for i in range(n):\n",
    "            # for each iteration, randomly hold out 20% of the data as CV set\n",
    "            X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(\n",
    "            df_X, df_y, test_size=.30, random_state=i * SEED)\n",
    "\n",
    "            # train model and make predictions\n",
    "            clf.fit(X_train, y_train) \n",
    "            preds = clf.predict_proba(X_cv)[:, 1]\n",
    "\n",
    "            # compute AUC metric for this CV fold\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_cv, preds)\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            print (\"AUC (fold %d/%d): %f\" % (i + 1, n, roc_auc))\n",
    "            mean_auc.append(roc_auc )\n",
    "            \n",
    "    elif algo == 'nn':\n",
    "        mean_auc = []\n",
    "        value_array = df_new.values\n",
    "        y = value_array[:, -1]\n",
    "        X = value_array[:, :-1]\n",
    "        \n",
    "        SEED=43\n",
    "        n = 4\n",
    "        for i in range(n):\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "            X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(\n",
    "                X, y, test_size=.30, random_state=i * SEED)\n",
    "            sc = StandardScaler()\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "            X_cv = sc.transform(X_cv)\n",
    "            classifier = Sequential()\n",
    "            # Adding the input layer and the first hidden layer\n",
    "            classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = len(X[1])))\n",
    "            # Adding the second hidden layer\n",
    "            classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "            # Adding the output layer\n",
    "            classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "            # Compiling Neural Network\n",
    "            classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "            # Fitting our model \n",
    "            classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 20)\n",
    "            y_pred = classifier.predict(X_cv)\n",
    "            y_pred = (y_pred > 0.5)\n",
    "            nn_roc_auc = roc_auc_score(y_cv, y_pred)\n",
    "            mean_auc.append(nn_roc_auc)\n",
    "\n",
    "    print (\"Mean AUC: %f\" % ( np.mean(mean_auc)))\n",
    "    print (\"std AUC: %f\" % np.std(mean_auc))\n",
    "    return (np.mean(mean_auc),np.std(mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_list = []\n",
    "lg_list = []\n",
    "\n",
    "\n",
    "for i in top_range:\n",
    "    print('currently running top: ' + str(i))\n",
    "    \n",
    "    rf_result = auc_machine(dfs_2008_test_copy,i, 'rf')\n",
    "    rf_list.append(rf_result)\n",
    "    \n",
    "    lg_result = auc_machine(dfs_2008_test_copy,i, 'lg')\n",
    "    lg_list.append(lg_result)\n",
    "    \n",
    "    #lg_list = lf_auc(dfs_2008_test_copy,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_list = []\n",
    "start_time = time.time\n",
    "\n",
    "for i in top_range:\n",
    "    print('currently running top: ' + str(i))\n",
    "    nn_result = auc_machine(dfs_2008_test_copy,i, 'nn')\n",
    "    nn_list.append(nn_result)\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rf_list[1])\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "rf_auc = []\n",
    "rf_err = []\n",
    "lg_auc = []\n",
    "lg_err = []\n",
    "nn_auc = []\n",
    "nn_err = []\n",
    "\n",
    "for i in rf_list:\n",
    "    rf_auc.append(i[0])\n",
    "    rf_err.append(i[1])\n",
    "    \n",
    "for i in lg_list:\n",
    "    lg_auc.append(i[0])\n",
    "    lg_err.append(i[1])\n",
    "    \n",
    "for i in nn_list:\n",
    "    nn_auc.append(i[0])\n",
    "    nn_err.append(i[1])\n",
    "    \n",
    "plt.xlabel(\"Default top percentage\")\n",
    "plt.ylabel(\"modified auc\")\n",
    "plt.title(\"top-pct to auc\")\n",
    "\n",
    "plt.errorbar(top_range,rf_auc,yerr=rf_err, fmt='o')\n",
    "plt.errorbar(top_range,lg_auc,yerr=lg_err, fmt='o')\n",
    "plt.errorbar(top_range,nn_auc,yerr=nn_err, fmt='o')\n",
    "plt.plot(top_range,rf_auc,label = 'rf_auc')\n",
    "plt.plot(top_range,lg_auc,label = 'lg_auc')\n",
    "plt.plot(top_range,nn_auc,label = 'nn_auc')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
