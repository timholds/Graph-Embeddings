{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set(rc={'figure.figsize':(12,8)})\n",
    "# sns.set(style='whitegrid')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# public_address = '54.174.175.98'\n",
    "# public_address = '54.88.167.164'\n",
    "public_address = 'top42_neo4j'\n",
    "\n",
    "graph = Graph('bolt://{}:7687'.format(public_address), auth=('neo4j','myneo'))\n",
    "\n",
    "def run_query(query, graph, print_query=False, run_query=True, \n",
    "              print_only=False, to_df=False, verbose=True):\n",
    "    df = 1\n",
    "    if print_only: \n",
    "        print_query = True\n",
    "        run_query = False\n",
    "    start_time = time.time()\n",
    "    if print_query:\n",
    "        print(query)\n",
    "    if run_query:\n",
    "        if to_df:\n",
    "            df = graph.run(query).to_data_frame()\n",
    "        else:\n",
    "            graph.run(query)\n",
    "    end_time = time.time()\n",
    "    minutes_elapsed = (end_time-start_time)/60\n",
    "    if verbose:\n",
    "        print(\"Query completed in {:.2f} minutes.\".format(minutes_elapsed))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_consider = range(1950, 2019)\n",
    "max_years_to_predict = 10\n",
    "years_tracked = 5\n",
    "\n",
    "vars_to_use = ['adopters', 'timeScaledPageRank', 'citations', 'node2vec']\n",
    "\n",
    "max_year = max(years_to_consider)\n",
    "min_year = min(years_to_consider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CALL apoc.export.csv.query('\n",
      "\n",
      "MATCH (a:Author)-[:AUTHORED]->(q:Quanta)-[:PUBLISHED_IN]->(y:Year)\n",
      "WHERE y.year>=1950 AND y.year<=2018//-10\n",
      "MATCH (q)-[m0:METRICS_IN]->(:Year {year:y.year+0})\n",
      "MATCH (q)-[m1:METRICS_IN]->(:Year {year:y.year+1})\n",
      "MATCH (q)-[m2:METRICS_IN]->(:Year {year:y.year+2})\n",
      "MATCH (q)-[m3:METRICS_IN]->(:Year {year:y.year+3})\n",
      "MATCH (q)-[m4:METRICS_IN]->(:Year {year:y.year+4})\n",
      "MATCH (q)-[m5:METRICS_IN]->(:Year {year:y.year+5})\n",
      "\n",
      "WHERE exists(m0.node2vec) AND exists(m1.node2vec)\n",
      "RETURN \n",
      "    coalesce(m0.adopters,0) AS adopters_0,\n",
      "coalesce(m0.timeScaledPageRank,0) AS timeScaledPageRank_0,\n",
      "coalesce(m0.citations,0) AS citations_0,\n",
      "coalesce(m0.node2vec,0) AS node2vec_0,\n",
      "coalesce(m1.adopters,0) AS adopters_1,\n",
      "coalesce(m1.timeScaledPageRank,0) AS timeScaledPageRank_1,\n",
      "coalesce(m1.citations,0) AS citations_1,\n",
      "coalesce(m1.node2vec,0) AS node2vec_1,\n",
      "coalesce(m2.adopters,0) AS adopters_2,\n",
      "coalesce(m2.timeScaledPageRank,0) AS timeScaledPageRank_2,\n",
      "coalesce(m2.citations,0) AS citations_2,\n",
      "coalesce(m2.node2vec,0) AS node2vec_2,\n",
      "coalesce(m3.adopters,0) AS adopters_3,\n",
      "coalesce(m3.timeScaledPageRank,0) AS timeScaledPageRank_3,\n",
      "coalesce(m3.citations,0) AS citations_3,\n",
      "coalesce(m3.node2vec,0) AS node2vec_3,\n",
      "coalesce(m4.adopters,0) AS adopters_4,\n",
      "coalesce(m4.timeScaledPageRank,0) AS timeScaledPageRank_4,\n",
      "coalesce(m4.citations,0) AS citations_4,\n",
      "coalesce(m4.node2vec,0) AS node2vec_4,\n",
      "coalesce(m5.adopters,0) AS adopters_5,\n",
      "coalesce(m5.timeScaledPageRank,0) AS timeScaledPageRank_5,\n",
      "coalesce(m5.citations,0) AS citations_5,\n",
      "coalesce(m5.node2vec,0) AS node2vec_5,\n",
      "    \n",
      "    id(q) AS id, \n",
      "    y.year AS year\n",
      "\n",
      "','/import/quanta.predict.1950.2018.csv', \n",
      "{quotes:true});\n",
      "\n",
      "Query completed in 0.00 minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_pattern = 'MATCH (q)-[m{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "metrics_string_list = [metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "metrics_string = '\\n'.join(metrics_string_list)\n",
    "\n",
    "var_pattern = 'coalesce(m{y}.{v},0) AS {v}_{y},'\n",
    "var_string_list = [var_pattern.format(y=i,v=j) for i in range(years_tracked+1) for j in vars_to_use]\n",
    "var_string = '\\n'.join(var_string_list)\n",
    "\n",
    "where_pattern = 'exists(m{y}.node2vec)'\n",
    "where_string_list = [where_pattern.format(y=i) for i in range(2)]#years_tracked+1)]\n",
    "where_string = ' AND '.join(where_string_list)       \n",
    "\n",
    "author_metrics_pattern = 'MATCH (q)-[ma{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "author_metrics_string_list = [author_metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "author_metrics_string = '\\n'.join(author_metrics_string_list)\n",
    "\n",
    "author_vars = ['total_papers', 'author_age', 'max_citations', 'num_venues', 'total_citations']\n",
    "author_funs = ['max', 'sum']\n",
    "author_var_pattern = '{f}(coalesce(ma{y}.{v},0)) AS {v}_{f}_{y},'\n",
    "author_var_string_list = [author_var_pattern.format(y=i, v=j, f=k)\n",
    "                          for i in range(years_tracked+1)\n",
    "                          for j in author_vars\n",
    "                          for k in author_funs]\n",
    "author_var_string = '\\n'.join(author_var_string_list)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Author)-[:AUTHORED]->(q:Quanta)-[:PUBLISHED_IN]->(y:Year)\n",
    "WHERE y.year>={miny} AND y.year<={maxy}//-{ya}\n",
    "{metrics_string}\n",
    "{author_metrics_string}\n",
    "WHERE {where_string}\n",
    "RETURN \n",
    "    {var_string}\n",
    "    {author_var_string}\n",
    "    id(q) AS id, \n",
    "    y.year AS year\n",
    "\"\"\".format(miny=min_year, \n",
    "           maxy=max_year,\n",
    "           ya=max_years_to_predict,\n",
    "           metrics_string=metrics_string,\n",
    "           var_string=var_string, \n",
    "           where_string=where_string, \n",
    "           author_metrics_string='',\n",
    "           author_var_string='')\n",
    "\n",
    "query_tocsv = \"\"\"\n",
    "CALL apoc.export.csv.query('\n",
    "{q}\n",
    "','/import/quanta.predict.{miny}.{maxy}.{yt}.csv', \n",
    "{{quotes:true}});\n",
    "\"\"\".format(q=query, \n",
    "           miny=min_year, \n",
    "           maxy=max_year,\n",
    "           yt=years_tracked)\n",
    "\n",
    "run_query(query_tocsv, graph, to_df=False, print_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data To CSV (With Author Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex = \"\"\"\n",
    "MATCH (q)-[:PUBLISHED_IN]->(y:year {{year: {}}}) \n",
    "MATCH (q)-[m:METRICS_IN]->(y)\n",
    "MATCH (a)-[:AUTHORED]-(q)\n",
    "MATCH ()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "metrics_pattern = 'MATCH (q)-[m{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "metrics_string_list = [metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "metrics_string = '\\n'.join(metrics_string_list)\n",
    "\n",
    "var_pattern = 'coalesce(m{y}.{v},0) AS {v}_{y},'\n",
    "var_string_list = [var_pattern.format(y=i,v=j) for i in range(years_tracked+1) for j in vars_to_use]\n",
    "var_string = '\\n'.join(var_string_list)\n",
    "\n",
    "where_pattern = 'exists(m{y}.node2vec)'\n",
    "where_string_list = [where_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "where_string = ' AND '.join(where_string_list)       \n",
    "\n",
    "author_metrics_pattern = 'MATCH (q)<-[:AUTHORED]-(a:Author)-[ma{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})' #changed from q to a\n",
    "author_metrics_string_list = [author_metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "author_metrics_string = '\\n'.join(author_metrics_string_list)\n",
    "\n",
    "# node2vec with different nodes types\n",
    "# change only to average\n",
    "#author_vars = ['total_papers', 'author_age', 'max_citations', 'num_venues', 'total_citations']\n",
    "author_vars = [\n",
    "    'hIndex', 'hIndexDelta', 'totalCitations', 'totalCitationsDelta', 'citationsPerPaper', \n",
    "    'citationsPerPaperDelta','citationsPerYear', 'totalPapers', 'totalPapersDelta', 'rankCitationsPerYear',\n",
    "    'pageRank', 'weightedPageRank', 'authorAge', 'recentCoauthors', 'maxCitations', 'totalVenues', \n",
    "    'venueHIndexMean', 'venueHIndexDeltaMean', 'venueCitationsPerPaperMean', 'venueCitationsPerPaperDeltaMean',  \n",
    "    'venueTotalPapersMean', 'venueTotalPapersDeltaMean', 'venueRankCitationsPerPaperMean', 'venueMaxCitationsMean']\n",
    "\n",
    "author_funs = ['avg', 'max', 'sum'] # why are we doing sum here? should we do average instead or in addition to sum?\n",
    "author_var_patterns = \n",
    "author_var_pattern = '{f}(coalesce(ma{y}.{v},0)) AS {v}_{f}_{y},'\n",
    "author_var_string_list = [author_var_pattern.format(y=i, v=j, f=k)\n",
    "                          for i in range(years_tracked+1)\n",
    "                          for j in author_vars\n",
    "                          for k in author_funs]\n",
    "author_var_string = '\\n'.join(author_var_string_list)\n",
    "\n",
    "# Do we need any with statements\n",
    "query = \"\"\"\n",
    "MATCH (a:Author)-[:AUTHORED]->(q:Quanta)-[:PUBLISHED_IN]->(y:Year)\n",
    "WHERE y.year>={miny} AND y.year<={maxy}//-{ya}\n",
    "{metrics_string}\n",
    "{author_metrics_string}\n",
    "WHERE {where_string}\n",
    "RETURN \n",
    "    {var_string}\n",
    "    {author_var_string}\n",
    "    id(q) AS id, \n",
    "    y.year AS year\n",
    "\"\"\".format(miny=min_year, \n",
    "           maxy=max_year,\n",
    "           ya=max_years_to_predict,\n",
    "           metrics_string=metrics_string,\n",
    "           var_string=var_string, \n",
    "           where_string=where_string, \n",
    "           author_metrics_string='',\n",
    "           author_var_string='')\n",
    "\n",
    "query_tocsv = \"\"\"\n",
    "CALL apoc.export.csv.query('\n",
    "{q}\n",
    "','/import/quanta.predict.{miny}.{maxy}.{yt}.csv', \n",
    "{{quotes:true}});\n",
    "\"\"\".format(q=query, \n",
    "           miny=min_year, \n",
    "           maxy=max_year,\n",
    "           yt=years_tracked)\n",
    "\n",
    "run_query(query_tocsv, graph, to_df=False, print_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data for each year, format it, and write it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = '/tmp/data/quanta.predict.{miny}.{maxy}.{yt}.csv'.format(\n",
    "    miny=min_year, maxy=max_year, yt=years_tracked)\n",
    "df = pd.read_csv(fpath, engine='python', error_bad_lines=False)\n",
    "df = df.dropna()\n",
    "\n",
    "for y in tqdm(range(years_tracked+1)):\n",
    "\n",
    "    col = 'node2vec_{}'.format(y)    \n",
    "    n2vdf = pd.DataFrame(df[col].apply(json.loads).tolist())\n",
    "    n2v_dim = n2vdf.shape[1]\n",
    "    n2vdf.columns = ['{}_{}'.format(col, i) for i in range(n2v_dim)]\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), n2vdf.reset_index(drop=True)], axis=1)\n",
    "    df = df.drop(col, axis=1)\n",
    "\n",
    "\n",
    "df.to_csv('{}.out'.format(fpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in prediction data and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, validation_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, mean_squared_error, classification_report, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/tmp/data/quanta.predict.{miny}.{maxy}.{yt}.csv.out'.format(\n",
    "    miny=min_year, maxy=max_year, yt=years_tracked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>adopters_0</th>\n",
       "      <th>adopters_1</th>\n",
       "      <th>adopters_2</th>\n",
       "      <th>adopters_3</th>\n",
       "      <th>adopters_4</th>\n",
       "      <th>adopters_5</th>\n",
       "      <th>citations_0</th>\n",
       "      <th>citations_1</th>\n",
       "      <th>citations_2</th>\n",
       "      <th>...</th>\n",
       "      <th>node2vec_5_2</th>\n",
       "      <th>node2vec_5_3</th>\n",
       "      <th>node2vec_5_4</th>\n",
       "      <th>node2vec_5_5</th>\n",
       "      <th>node2vec_5_6</th>\n",
       "      <th>node2vec_5_7</th>\n",
       "      <th>node2vec_5_8</th>\n",
       "      <th>node2vec_5_9</th>\n",
       "      <th>node2vec_5_10</th>\n",
       "      <th>node2vec_5_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "      <td>5.262588e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.631294e+06</td>\n",
       "      <td>1.132223e+01</td>\n",
       "      <td>2.889139e+01</td>\n",
       "      <td>3.120092e+01</td>\n",
       "      <td>2.497449e+01</td>\n",
       "      <td>5.485345e-01</td>\n",
       "      <td>7.468759e-02</td>\n",
       "      <td>8.941316e-01</td>\n",
       "      <td>2.060872e+00</td>\n",
       "      <td>2.016791e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.420117e+12</td>\n",
       "      <td>9.878786e+12</td>\n",
       "      <td>7.043323e+13</td>\n",
       "      <td>-5.330409e+13</td>\n",
       "      <td>-1.397789e+13</td>\n",
       "      <td>1.691927e+12</td>\n",
       "      <td>2.406043e+13</td>\n",
       "      <td>3.138254e+13</td>\n",
       "      <td>9.032189e+13</td>\n",
       "      <td>-1.366777e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.519178e+06</td>\n",
       "      <td>6.883398e+01</td>\n",
       "      <td>1.196610e+02</td>\n",
       "      <td>1.421127e+02</td>\n",
       "      <td>1.193722e+02</td>\n",
       "      <td>3.936234e+01</td>\n",
       "      <td>5.916535e+00</td>\n",
       "      <td>2.479747e+00</td>\n",
       "      <td>4.744520e+00</td>\n",
       "      <td>4.934994e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.228760e+14</td>\n",
       "      <td>1.632540e+14</td>\n",
       "      <td>7.140607e+14</td>\n",
       "      <td>5.624136e+14</td>\n",
       "      <td>2.831408e+14</td>\n",
       "      <td>5.939874e+13</td>\n",
       "      <td>5.096146e+14</td>\n",
       "      <td>7.109472e+14</td>\n",
       "      <td>7.441738e+14</td>\n",
       "      <td>1.055877e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.462150e+15</td>\n",
       "      <td>-5.927330e+15</td>\n",
       "      <td>-1.654170e+16</td>\n",
       "      <td>-3.072070e+16</td>\n",
       "      <td>-1.475570e+16</td>\n",
       "      <td>-7.837340e+15</td>\n",
       "      <td>-1.293990e+16</td>\n",
       "      <td>-1.804400e+16</td>\n",
       "      <td>-1.705340e+16</td>\n",
       "      <td>-6.349120e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.315647e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.173800e-01</td>\n",
       "      <td>-3.903130e-01</td>\n",
       "      <td>-2.209800e-01</td>\n",
       "      <td>-8.322860e-01</td>\n",
       "      <td>-8.384342e-01</td>\n",
       "      <td>-4.416950e-01</td>\n",
       "      <td>-3.936530e-01</td>\n",
       "      <td>-1.720010e-01</td>\n",
       "      <td>-5.172780e-01</td>\n",
       "      <td>-7.182760e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.631294e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.850575e-02</td>\n",
       "      <td>-2.689320e-02</td>\n",
       "      <td>8.682170e-03</td>\n",
       "      <td>-5.328800e-02</td>\n",
       "      <td>-3.869480e-02</td>\n",
       "      <td>-3.606940e-02</td>\n",
       "      <td>-2.413070e-02</td>\n",
       "      <td>7.077720e-02</td>\n",
       "      <td>4.356530e-02</td>\n",
       "      <td>-9.097480e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.946940e+06</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.496510e-01</td>\n",
       "      <td>1.934150e-01</td>\n",
       "      <td>2.613402e-01</td>\n",
       "      <td>1.444350e-01</td>\n",
       "      <td>1.796960e-01</td>\n",
       "      <td>1.637490e-01</td>\n",
       "      <td>4.282600e-01</td>\n",
       "      <td>5.274802e-01</td>\n",
       "      <td>4.511310e-01</td>\n",
       "      <td>3.999810e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.262587e+06</td>\n",
       "      <td>2.992000e+03</td>\n",
       "      <td>5.025000e+03</td>\n",
       "      <td>5.025000e+03</td>\n",
       "      <td>4.999000e+03</td>\n",
       "      <td>4.015000e+03</td>\n",
       "      <td>7.730000e+02</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>2.050000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>5.361910e+15</td>\n",
       "      <td>7.120870e+15</td>\n",
       "      <td>3.887430e+16</td>\n",
       "      <td>1.355800e+16</td>\n",
       "      <td>3.369140e+16</td>\n",
       "      <td>3.422790e+15</td>\n",
       "      <td>5.631000e+16</td>\n",
       "      <td>7.780440e+16</td>\n",
       "      <td>4.669420e+16</td>\n",
       "      <td>2.350950e+16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0    adopters_0    adopters_1    adopters_2    adopters_3  \\\n",
       "count  5.262588e+06  5.262588e+06  5.262588e+06  5.262588e+06  5.262588e+06   \n",
       "mean   2.631294e+06  1.132223e+01  2.889139e+01  3.120092e+01  2.497449e+01   \n",
       "std    1.519178e+06  6.883398e+01  1.196610e+02  1.421127e+02  1.193722e+02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    1.315647e+06  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    2.631294e+06  0.000000e+00  3.000000e+00  3.000000e+00  0.000000e+00   \n",
       "75%    3.946940e+06  4.000000e+00  2.000000e+01  2.100000e+01  1.600000e+01   \n",
       "max    5.262587e+06  2.992000e+03  5.025000e+03  5.025000e+03  4.999000e+03   \n",
       "\n",
       "         adopters_4    adopters_5   citations_0   citations_1   citations_2  \\\n",
       "count  5.262588e+06  5.262588e+06  5.262588e+06  5.262588e+06  5.262588e+06   \n",
       "mean   5.485345e-01  7.468759e-02  8.941316e-01  2.060872e+00  2.016791e+00   \n",
       "std    3.936234e+01  5.916535e+00  2.479747e+00  4.744520e+00  4.934994e+00   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  1.000000e+00  2.000000e+00  2.000000e+00   \n",
       "max    4.015000e+03  7.730000e+02  9.900000e+01  1.800000e+02  2.050000e+02   \n",
       "\n",
       "           ...        node2vec_5_2  node2vec_5_3  node2vec_5_4  node2vec_5_5  \\\n",
       "count      ...        5.262588e+06  5.262588e+06  5.262588e+06  5.262588e+06   \n",
       "mean       ...        7.420117e+12  9.878786e+12  7.043323e+13 -5.330409e+13   \n",
       "std        ...        1.228760e+14  1.632540e+14  7.140607e+14  5.624136e+14   \n",
       "min        ...       -4.462150e+15 -5.927330e+15 -1.654170e+16 -3.072070e+16   \n",
       "25%        ...       -1.173800e-01 -3.903130e-01 -2.209800e-01 -8.322860e-01   \n",
       "50%        ...        3.850575e-02 -2.689320e-02  8.682170e-03 -5.328800e-02   \n",
       "75%        ...        5.496510e-01  1.934150e-01  2.613402e-01  1.444350e-01   \n",
       "max        ...        5.361910e+15  7.120870e+15  3.887430e+16  1.355800e+16   \n",
       "\n",
       "       node2vec_5_6  node2vec_5_7  node2vec_5_8  node2vec_5_9  node2vec_5_10  \\\n",
       "count  5.262588e+06  5.262588e+06  5.262588e+06  5.262588e+06   5.262588e+06   \n",
       "mean  -1.397789e+13  1.691927e+12  2.406043e+13  3.138254e+13   9.032189e+13   \n",
       "std    2.831408e+14  5.939874e+13  5.096146e+14  7.109472e+14   7.441738e+14   \n",
       "min   -1.475570e+16 -7.837340e+15 -1.293990e+16 -1.804400e+16  -1.705340e+16   \n",
       "25%   -8.384342e-01 -4.416950e-01 -3.936530e-01 -1.720010e-01  -5.172780e-01   \n",
       "50%   -3.869480e-02 -3.606940e-02 -2.413070e-02  7.077720e-02   4.356530e-02   \n",
       "75%    1.796960e-01  1.637490e-01  4.282600e-01  5.274802e-01   4.511310e-01   \n",
       "max    3.369140e+16  3.422790e+15  5.631000e+16  7.780440e+16   4.669420e+16   \n",
       "\n",
       "       node2vec_5_11  \n",
       "count   5.262588e+06  \n",
       "mean   -1.366777e+14  \n",
       "std     1.055877e+15  \n",
       "min    -6.349120e+16  \n",
       "25%    -7.182760e-01  \n",
       "50%    -9.097480e-02  \n",
       "75%     3.999810e-01  \n",
       "max     2.350950e+16  \n",
       "\n",
       "[8 rows x 93 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([      0,       1,       2,       3,       4,       5,       6,\n",
       "                  7,       8,       9,\n",
       "            ...\n",
       "            5262578, 5262579, 5262580, 5262581, 5262582, 5262583, 5262584,\n",
       "            5262585, 5262586, 5262587],\n",
       "           dtype='int64', length=5262588)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index[df['year'] <= max_year - years_tracked + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for years_to_track in tqdm(range(years_tracked)):\n",
    "#     for year_to_predict in range(years_to_track+1, max_years_to_predict+1):\n",
    "#         print(years_to_track, year_to_predict)\n",
    "#         pass\n",
    "    year_to_predict = years_to_track + 1\n",
    "    \n",
    "    cols_to_keep = ['{v}_{y}'.format(y=i,v=j) \n",
    "        for i in range(years_to_track+1) \n",
    "        for j in [v for v in vars_to_use if v!='node2vec']]\n",
    "\n",
    "    n2v_cols_to_keep =  ['node2vec_{y}_{i}'.format(y=y, i=i) \n",
    "                     for y in range(years_to_track+1)\n",
    "                     for i in range(n2v_dim)]\n",
    "        \n",
    "    cols_to_keep = cols_to_keep + n2v_cols_to_keep\n",
    "    X = df.loc[:, cols_to_keep]\n",
    "    \n",
    "    y_col = 'timeScaledPageRank_{y}'.format(y=year_to_predict)\n",
    "    y = df.loc[:, y_col] > df[y_col].quantile(q=.95)\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        memory=None,\n",
    "        steps=[\n",
    "            ('spl', SMOTE()),\n",
    "            ('scl', QuantileTransformer()),\n",
    "            ('clf', RandomForestClassifier())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    grid = {'clf__n_estimators': [int(x) for x in np.linspace(200, 2000, num=10)],\n",
    "            'clf__max_features': ['auto', 'sqrt'],\n",
    "            'clf__max_depth': [int(x) for x in np.linspace(10, 1000, num=10)],\n",
    "            'clf__min_samples_split': [2, 5, 10],\n",
    "            'clf__min_samples_leaf': [1, 2, 4],\n",
    "            'clf__bootstrap': [True, False]}\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline, \n",
    "        param_distributions=grid, \n",
    "        n_iter=10, \n",
    "        cv=3, \n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    with parallel_backend('threading'):\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = random_search.predict(X_test)\n",
    "\n",
    "    results.append({\n",
    "        'years_tracked': years_to_track, \n",
    "        'year_predicted': year_to_predict,\n",
    "        'score': random_search.score(X=X_test, y=y_test),\n",
    "        'f1': f1_score(y_pred=y_pred, y_true=y_test),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n",
    "        'balanced_accuracy_adjusted': balanced_accuracy_score(y_true=y_test, y_pred=y_pred, adjusted=True),\n",
    "        'classification_report': classification_report(y_true=y_test, y_pred=y_pred, output_dict=True),\n",
    "        'random_search': random_search\n",
    "    })\n",
    "\n",
    "    pickle.dump(results, open('prediction_models_{}.pickle'.format(years_to_track),'wb'))\n",
    "    \n",
    "pickle.dump(results, open('prediction_models.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_track=3\n",
    "year_to_predict=5\n",
    "n2v_dim=12\n",
    "\n",
    "results = []\n",
    "\n",
    "cols_to_keep = ['{v}_{y}'.format(y=i,v=j) \n",
    "        for i in range(years_to_track+1) \n",
    "        for j in [v for v in vars_to_use if v!='node2vec']]\n",
    "\n",
    "n2v_cols_to_keep =  ['node2vec_{y}_{i}'.format(y=y, i=i) \n",
    "                 for y in range(years_to_track+1)\n",
    "                 for i in range(n2v_dim)]\n",
    "\n",
    "cols_to_keep = cols_to_keep + n2v_cols_to_keep\n",
    "X = df.loc[:, cols_to_keep]\n",
    "\n",
    "y_col = 'timeScaledPageRank_{y}'.format(y=year_to_predict)\n",
    "y = df.loc[:, y_col] > df[y_col].quantile(q=.95)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    memory=None,\n",
    "    steps=[\n",
    "        ('spl', RandomUnderSampler()),\n",
    "        ('scl', QuantileTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "grid = {'clf__n_estimators': [int(x) for x in np.linspace(200, 2000, num=10)],\n",
    "        'clf__max_features': ['auto', 'sqrt'],\n",
    "        'clf__max_depth': [int(x) for x in np.linspace(10, 1000, num=10)],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__bootstrap': [True, False]}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_distributions=grid, \n",
    "    n_iter=15, \n",
    "    cv=3, \n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "y_pred = random_search.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'years_tracked': years_to_track, \n",
    "    'year_predicted': year_to_predict,\n",
    "    'score': random_search.score(X=X_test, y=y_test),\n",
    "    'f1': f1_score(y_pred=y_pred, y_true=y_test),\n",
    "    'balanced_accuracy': balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n",
    "    'balanced_accuracy_adjusted': balanced_accuracy_score(y_true=y_test, y_pred=y_pred, adjusted=True),\n",
    "    'classification_report': classification_report(y_true=y_test, y_pred=y_pred, output_dict=True),\n",
    "    'random_search': random_search\n",
    "})\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_track=1\n",
    "year_to_predict=10\n",
    "n2v_dim=12\n",
    "\n",
    "results = pd.DataFrame(columns=['years_tracked', 'year_predicted', 'score'])\n",
    "\n",
    "\n",
    "vars_to_use = ['adopters', 'citatons', 'node2vec']\n",
    "\n",
    "cols_to_keep = ['{v}_{y}'.format(y=i,v=j) \n",
    "        for i in range(years_to_track+1) \n",
    "        for j in [v for v in vars_to_use if v!='node2vec']]\n",
    "\n",
    "n2v_cols_to_keep =  ['node2vec_{y}_{i}'.format(y=y, i=i) \n",
    "                 for y in range(years_to_track+1)\n",
    "                 for i in range(n2v_dim)]\n",
    "\n",
    "cols_to_keep = cols_to_keep + n2v_cols_to_keep\n",
    "X = df.loc[:, cols_to_keep]\n",
    "\n",
    "y_col = ['timeScaledPageRank_{y}'.format(y=i) for i in range(2,10)]\n",
    "\n",
    "\n",
    "y = df.loc[:, y_col] > df[y_col].quantile(q=.9)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [10, None],\n",
    "    \"max_features\": [5, 10, 20],\n",
    "    \"n_estimators\": [10, 100, 200]\n",
    "}\n",
    "\n",
    "# pipeline = make_pipeline(\n",
    "# #     SMOTE(),\n",
    "#     RandomUnderSampler(),\n",
    "# #     QuantileTransformer(),\n",
    "#     MinMaxScaler(),\n",
    "#     GridSearchCV(RandomForestClassifier(class_weight={True:3, False:1}),\n",
    "#                  param_grid=param_grid, \n",
    "#                  refit=True, \n",
    "#                  n_jobs=-1, \n",
    "#                  cv=2, \n",
    "#                  scoring='f1') \n",
    "# )\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    memory=None,\n",
    "    steps=[\n",
    "#         ('spl', SMOTE()),\n",
    "#         ('scl', MinMaxScaler()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "results = results.append({\n",
    "    'years_tracked': years_to_track, \n",
    "    'year_predicted': year_to_predict,\n",
    "    'score': pipeline.score(X=X_test, y=y_test),\n",
    "#     'f1': f1_score(y_pred=y_pred, y_true=y_test)\n",
    "}, ignore_index=True)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict_proba(df.loc[df['id'] == 29052283, cols_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_metric.functions import  BinaryClassification\n",
    "\n",
    "bc = BinaryClassification(y_test, y_prob, labels=[\"Low Impact\", \"High Impact\"])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\n",
    "bc.plot_roc_curve()\n",
    "\n",
    "plt.subplot2grid((2,6), (0,2), colspan=2)\n",
    "bc.plot_precision_recall_curve()\n",
    "\n",
    "plt.subplot2grid((2,6), (0,4), colspan=2)\n",
    "bc.plot_class_distribution()\n",
    "\n",
    "plt.subplot2grid((2,6), (1,1), colspan=2)\n",
    "bc.plot_confusion_matrix()\n",
    "\n",
    "plt.subplot2grid((2,6), (1,3), colspan=2)\n",
    "bc.plot_confusion_matrix(normalize=True)\n",
    "\n",
    "plt.show()\n",
    "bc.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "f1_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=results.pivot('years_tracked', 'year_predicted', 'score'), \n",
    "           annot=True, fmt='.2f', linewidth=.5, cbar=True, square=True, \n",
    "           cmap='YlGnBu', center=results['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "feature_importance = pd.DataFrame.from_dict(\n",
    "    dict(zip(X.columns,pipeline.steps[1][1].feature_importances_)), orient='index').T\n",
    "sns.barplot(orient='h',data=feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param = 'randomforestclassifier__max_depth'\n",
    "param_range = list(range(1,100,25))\n",
    "n_cv = 2\n",
    "\n",
    "train_scores, valid_scores = validation_curve(pipeline, \n",
    "                                              X=X, \n",
    "                                              y=y,\n",
    "                                              n_jobs=-1,\n",
    "                                              param_name=param,\n",
    "                                              scoring='roc_auc',\n",
    "                                              param_range=param_range,\n",
    "                                              cv=n_cv)\n",
    "\n",
    "vdf = pd.DataFrame(np.concatenate([train_scores, valid_scores]),\n",
    "             columns=['cv_fold_{}'.format(i) for i in range(n_cv)],\n",
    "            )\n",
    "vdf[param] = param_range*2\n",
    "vdf['type'] = ['train']*len(param_range) + ['valid']*len(param_range)\n",
    "\n",
    "sns.lineplot(data=vdf.melt(id_vars=['type', param]), \n",
    "             x=param, y='value', hue='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train, X_test]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2, n_jobs=-1, \n",
    "                                                    scoring='r2', config_dict='TPOT light', \n",
    "                                                    max_time_mins=30, max_eval_time_mins=5) \n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tpot_exported_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_ahead = 4\n",
    "df = pd.read_csv('/tmp/data/quanta.predict.{miny}.{maxy}.{ya}.csv.out'.format(\n",
    "        miny=min_year, maxy=max_year, ya=years_ahead))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlbox.preprocessing import *\n",
    "from mlbox.optimisation import *\n",
    "from mlbox.prediction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
