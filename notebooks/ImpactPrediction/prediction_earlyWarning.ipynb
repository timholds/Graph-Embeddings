{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# public_address = '54.174.175.98'\n",
    "public_address = '18.85.22.109'\n",
    "\n",
    "graph = Graph('bolt://{}:7687'.format(public_address), auth=('neo4j','myneo'))\n",
    "\n",
    "def run_query(query, graph, print_query=False, run_query=True, \n",
    "              print_only=False, to_df=False, verbose=True):\n",
    "    df = 1\n",
    "    if print_only: \n",
    "        print_query = True\n",
    "        run_query = False\n",
    "    start_time = time.time()\n",
    "    if print_query:\n",
    "        print(query)\n",
    "    if run_query:\n",
    "        if to_df:\n",
    "            df = graph.run(query).to_data_frame()\n",
    "        else:\n",
    "            graph.run(query)\n",
    "    end_time = time.time()\n",
    "    minutes_elapsed = (end_time-start_time)/60\n",
    "    if verbose:\n",
    "        print(\"Query completed in {:.2f} minutes.\".format(minutes_elapsed))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_consider = range(1900, 2020)\n",
    "max_years_to_predict = 10\n",
    "years_tracked = 10\n",
    "\n",
    "vars_to_use = ['adopters', 'timeScaledPageRank', 'citatons', 'node2vec']\n",
    "\n",
    "max_year = max(years_to_consider)\n",
    "min_year = min(years_to_consider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_pattern = 'MATCH (q)-[m{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "metrics_string_list = [metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "metrics_string = '\\n'.join(metrics_string_list)\n",
    "\n",
    "var_pattern = 'coalesce(m{y}.{v},0) AS {v}_{y},'\n",
    "var_string_list = [var_pattern.format(y=i,v=j) for i in range(years_tracked+1) for j in vars_to_use]\n",
    "var_string = '\\n'.join(var_string_list)\n",
    "\n",
    "where_pattern = 'exists(m{y}.node2vec)'\n",
    "where_string_list = [where_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "where_string = ' AND '.join(where_string_list)       \n",
    "\n",
    "author_metrics_pattern = 'MATCH (q)-[ma{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "author_metrics_string_list = [author_metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "author_metrics_string = '\\n'.join(author_metrics_string_list)\n",
    "\n",
    "author_vars = ['total_papers', 'author_age', 'max_citations', 'num_venues', 'total_citations']\n",
    "author_funs = ['max', 'sum']\n",
    "author_var_pattern = '{f}(coalesce(ma{y}.{v},0)) AS {v}_{f}_{y},'\n",
    "author_var_string_list = [author_var_pattern.format(y=i, v=j, f=k)\n",
    "                          for i in range(years_tracked+1)\n",
    "                          for j in author_vars\n",
    "                          for k in author_funs]\n",
    "author_var_string = '\\n'.join(author_var_string_list)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Author)-[:AUTHORED]->(q:Quanta)-[:PUBLISHED_IN]->(y:Year)\n",
    "WHERE y.year>={miny} AND y.year<={maxy}//-{ya}\n",
    "{metrics_string}\n",
    "{author_metrics_string}\n",
    "WHERE {where_string}\n",
    "RETURN \n",
    "    {var_string}\n",
    "    {author_var_string}\n",
    "    id(q) AS id, \n",
    "    y.year AS year\n",
    "\"\"\".format(miny=min_year, \n",
    "           maxy=max_year,\n",
    "           ya=max_years_to_predict,\n",
    "           metrics_string=metrics_string,\n",
    "           var_string=var_string, \n",
    "           where_string=where_string, \n",
    "           author_metrics_string='',\n",
    "           author_var_string='')\n",
    "\n",
    "query_tocsv = \"\"\"\n",
    "CALL apoc.export.csv.query('\n",
    "{q}\n",
    "','/import/quanta.predict.{miny}.{maxy}.csv', \n",
    "{{quotes:true}});\n",
    "\"\"\".format(q=query, \n",
    "           miny=min_year, \n",
    "           maxy=max_year,\n",
    "           yt=years_tracked)\n",
    "\n",
    "run_query(query_tocsv, graph, to_df=False, print_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data To CSV (With Author Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#author_vars = ['total_papers', 'author_age', 'max_citations', 'num_venues', 'total_citations']\n",
    "author_vars = [\n",
    "    'hIndex', 'hIndexDelta', 'totalCitations', 'totalCitationsDelta', 'citationsPerPaper', \n",
    "    'citationsPerPaperDelta','citationsPerYear', 'totalPapers', 'totalPapersDelta', 'rankCitationsPerYear',\n",
    "    'pageRank', 'weightedPageRank', 'authorAge', 'recentCoauthors', 'maxCitations', 'totalVenues', \n",
    "    'venueHIndexMean', 'venueHIndexDeltaMean', 'venueCitationsPerPaperMean', 'venueCitationsPerPaperDeltaMean',  \n",
    "    'venueTotalPapersMean', 'venueTotalPapersDeltaMean', 'venueRankCitationsPerPaperMean', 'venueMaxCitationsMean']\n",
    "\n",
    "\n",
    "metrics_pattern = 'MATCH (q)-[m{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "metrics_string_list = [metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "metrics_string = '\\n'.join(metrics_string_list)\n",
    "\n",
    "where_pattern = 'exists(m{y}.node2vec)'\n",
    "where_string_list = [where_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "where_string = ' AND '.join(where_string_list)   \n",
    "\n",
    "# figure out how to only get q and y once\n",
    "with_pattern = 'm{y}'\n",
    "with_string_list = [with_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "with_qy = 'WITH q, y, '\n",
    "with_string_part = ', '.join(with_string_list)\n",
    "with_string = with_qy + with_string_part\n",
    "\n",
    "author_metrics_pattern = 'MATCH (a)-[ma{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})' \n",
    "author_metrics_string_list = [author_metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "author_metrics_string = '\\n'.join(author_metrics_string_list)\n",
    "\n",
    "# figure out how to only get q and y once\n",
    "#with_pattern2 = 'WITH q, y, m{y}, collect(ma{y}.{v}) as ma{y}_{v} '\n",
    "#with_string2 = '\\n'.join(with_string_list)\n",
    "with_pattern2_part1 = with_string + ', '\n",
    "with_pattern2 = 'collect(ma{y}.{v}) as ma{y}_{v}'\n",
    "with_string_list2 = [with_pattern2.format(y=i, v=j) for i in range(years_tracked+1) for j in author_vars]\n",
    "with_pattern2_part2 = ', \\n '.join(with_string_list2)\n",
    "with_string2 = with_pattern2_part1 + with_pattern2_part2\n",
    "\n",
    "\n",
    "var_pattern = 'coalesce(m{y}.{v},0) AS {v}_{y},'\n",
    "var_string_list = [var_pattern.format(y=i,v=j) for i in range(years_tracked+1) for j in vars_to_use]\n",
    "var_string = '\\n'.join(var_string_list)\n",
    "    \n",
    "#author_funs = ['avg', 'max'] # are these the only things we want here?\n",
    "#author_var_pattern = '{f}(coalesce(apoc.coll.avg(ma{y}_{v}), 0)) AS {v}_{f}_{y},'\n",
    "author_var_pattern = 'coalesce(apoc.coll.avg(ma{y}_{v}), 0) AS {v}_{y}, \\n'\n",
    "\n",
    "author_var_string_list = [author_var_pattern.format(y=i, v=j) #, f=k)\n",
    "                          for i in range(years_tracked+1)\n",
    "                          for j in author_vars]\n",
    "                          #for k in author_funs]\n",
    "author_var_string = ''.join(author_var_string_list)\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (q:Quanta)-[:PUBLISHED_IN]->(y:Year)\n",
    "WHERE y.year>={miny} AND y.year<={maxy}//-{ya}\n",
    "{metrics_string}\n",
    "{where_string} \n",
    "{with_string}\n",
    "MATCH (q)<-[:AUTHORED]-(a:Author)\n",
    "{author_metrics_string}\n",
    "{with_string2}\n",
    "RETURN \n",
    "    {var_string}\n",
    "    {author_var_string}\n",
    "    id(q) AS id, \n",
    "    y.year AS year\n",
    "\"\"\".format(miny=min_year, \n",
    "           maxy=max_year,\n",
    "           ya=max_years_to_predict,\n",
    "           metrics_string=metrics_string,\n",
    "           where_string=where_string,\n",
    "           with_string=with_string,\n",
    "           with_string2=with_string2,\n",
    "           var_string=var_string, \n",
    "           author_metrics_string=author_metrics_string,\n",
    "           author_var_string=author_var_string)\n",
    "\n",
    "query_tocsv = \"\"\"\n",
    "CALL apoc.export.csv.query('\n",
    "{q}\n",
    "','/import/quanta.author.predict.{miny}.{maxy}.csv', \n",
    "{{quotes:true}});\n",
    "\"\"\".format(q=query, \n",
    "           miny=min_year, \n",
    "           maxy=max_year,\n",
    "           yt=years_tracked)\n",
    "\n",
    "run_query(query_tocsv, graph, to_df=False, print_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data for each year, format it, and write it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = '/tmp/data/quanta.predict.{miny}.{maxy}.{yt}.csv'.format(\n",
    "    miny=min_year, maxy=max_year, yt=years_tracked, ya=years_ahead)\n",
    "df = pd.read_csv(fpath, engine='python', error_bad_lines=False)\n",
    "df = df.dropna()\n",
    "\n",
    "for y in tqdm(range(years_tracked+1)):\n",
    "\n",
    "    col = 'node2vec_{}'.format(y)    \n",
    "    n2vdf = pd.DataFrame(df[col].apply(json.loads).tolist())\n",
    "    n2v_dim = n2vdf.shape[1]\n",
    "    n2vdf.columns = ['{}_{}'.format(col, i) for i in range(n2v_dim)]\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), n2vdf.reset_index(drop=True)], axis=1)\n",
    "    df = df.drop(col, axis=1)\n",
    "\n",
    "\n",
    "df.to_csv('{}.out'.format(fpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in prediction data and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, validation_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, mean_squared_error, classification_report, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/tmp/data/quanta.predict.{miny}.{maxy}.{yt}.csv.out'.format(\n",
    "    miny=min_year, maxy=max_year, yt=max_years_to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_years_to_predict = 10\n",
    "years_tracked = 6\n",
    "\n",
    "results = []\n",
    "\n",
    "for years_to_track in tqdm(range(years_tracked)):\n",
    "#     for year_to_predict in range(years_to_track+1, max_years_to_predict+1):\n",
    "#         print(years_to_track, year_to_predict)\n",
    "#         pass\n",
    "    year_to_predict = years_to_track + 1\n",
    "    \n",
    "    cols_to_keep = ['{v}_{y}'.format(y=i,v=j) \n",
    "        for i in range(years_to_track+1) \n",
    "        for j in [v for v in vars_to_use if v!='node2vec']]\n",
    "\n",
    "    n2v_cols_to_keep =  ['node2vec_{y}_{i}'.format(y=y, i=i) \n",
    "                     for y in range(years_to_track+1)\n",
    "                     for i in range(n2v_dim)]\n",
    "\n",
    "    cols_to_keep = cols_to_keep + n2v_cols_to_keep\n",
    "    X = df.loc[:, cols_to_keep]\n",
    "\n",
    "    y_col = 'timeScaledPageRank_{y}'.format(y=year_to_predict)\n",
    "    y = df.loc[:, y_col] > df[y_col].quantile(q=.95)\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        memory=None,\n",
    "        steps=[\n",
    "            ('spl', SMOTE()),\n",
    "            ('scl', QuantileTransformer()),\n",
    "            ('clf', RandomForestClassifier())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    grid = {'clf__n_estimators': [int(x) for x in np.linspace(200, 2000, num=10)],\n",
    "            'clf__max_features': ['auto', 'sqrt'],\n",
    "            'clf__max_depth': [int(x) for x in np.linspace(10, 1000, num=10)],\n",
    "            'clf__min_samples_split': [2, 5, 10],\n",
    "            'clf__min_samples_leaf': [1, 2, 4],\n",
    "            'clf__bootstrap': [True, False]}\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=pipeline, \n",
    "        param_distributions=grid, \n",
    "        n_iter=10, \n",
    "        cv=3, \n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    with parallel_backend('threading'):\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = random_search.predict(X_test)\n",
    "\n",
    "    results.append({\n",
    "        'years_tracked': years_to_track, \n",
    "        'year_predicted': year_to_predict,\n",
    "        'score': random_search.score(X=X_test, y=y_test),\n",
    "        'f1': f1_score(y_pred=y_pred, y_true=y_test),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n",
    "        'balanced_accuracy_adjusted': balanced_accuracy_score(y_true=y_test, y_pred=y_pred, adjusted=True),\n",
    "        'classification_report': classification_report(y_true=y_test, y_pred=y_pred, output_dict=True),\n",
    "        'random_search': random_search\n",
    "    })\n",
    "\n",
    "    pickle.dump(results, open('prediction_models_{}.pickle'.format(years_to_track),'wb'))\n",
    "    \n",
    "pickle.dump(results, open('prediction_models.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_track=3\n",
    "year_to_predict=4\n",
    "n2v_dim=12\n",
    "\n",
    "results = []\n",
    "\n",
    "cols_to_keep = ['{v}_{y}'.format(y=i,v=j) \n",
    "        for i in range(years_to_track+1) \n",
    "        for j in [v for v in vars_to_use if v!='node2vec']]\n",
    "\n",
    "n2v_cols_to_keep =  ['node2vec_{y}_{i}'.format(y=y, i=i) \n",
    "                 for y in range(years_to_track+1)\n",
    "                 for i in range(n2v_dim)]\n",
    "\n",
    "cols_to_keep = cols_to_keep + n2v_cols_to_keep\n",
    "X = df.loc[:, cols_to_keep]\n",
    "\n",
    "y_col = 'timeScaledPageRank_{y}'.format(y=year_to_predict)\n",
    "y = df.loc[:, y_col] > df[y_col].quantile(q=.95)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    memory=None,\n",
    "    steps=[\n",
    "        ('spl', RandomUnderSampler()),\n",
    "        ('scl', QuantileTransformer()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "grid = {'clf__n_estimators': [int(x) for x in np.linspace(200, 2000, num=10)],\n",
    "        'clf__max_features': ['auto', 'sqrt'],\n",
    "        'clf__max_depth': [int(x) for x in np.linspace(10, 1000, num=10)],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__bootstrap': [True, False]}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_distributions=grid, \n",
    "    n_iter=15, \n",
    "    cv=2, \n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "y_pred = random_search.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'years_tracked': years_to_track, \n",
    "    'year_predicted': year_to_predict,\n",
    "    'score': random_search.score(X=X_test, y=y_test),\n",
    "    'f1': f1_score(y_pred=y_pred, y_true=y_test),\n",
    "    'balanced_accuracy': balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n",
    "    'balanced_accuracy_adjusted': balanced_accuracy_score(y_true=y_test, y_pred=y_pred, adjusted=True),\n",
    "    'classification_report': classification_report(y_true=y_test, y_pred=y_pred, output_dict=True),\n",
    "    'random_search': random_search\n",
    "})\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_track=1\n",
    "year_to_predict=10\n",
    "n2v_dim=12\n",
    "\n",
    "results = pd.DataFrame(columns=['years_tracked', 'year_predicted', 'score'])\n",
    "\n",
    "\n",
    "vars_to_use = ['adopters', 'citatons', 'node2vec']\n",
    "\n",
    "cols_to_keep = ['{v}_{y}'.format(y=i,v=j) \n",
    "        for i in range(years_to_track+1) \n",
    "        for j in [v for v in vars_to_use if v!='node2vec']]\n",
    "\n",
    "n2v_cols_to_keep =  ['node2vec_{y}_{i}'.format(y=y, i=i) \n",
    "                 for y in range(years_to_track+1)\n",
    "                 for i in range(n2v_dim)]\n",
    "\n",
    "cols_to_keep = cols_to_keep + n2v_cols_to_keep\n",
    "X = df.loc[:, cols_to_keep]\n",
    "\n",
    "y_col = ['timeScaledPageRank_{y}'.format(y=i) for i in range(2,10)]\n",
    "\n",
    "\n",
    "y = df.loc[:, y_col] > df[y_col].quantile(q=.9)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [10, None],\n",
    "    \"max_features\": [5, 10, 20],\n",
    "    \"n_estimators\": [10, 100, 200]\n",
    "}\n",
    "\n",
    "# pipeline = make_pipeline(\n",
    "# #     SMOTE(),\n",
    "#     RandomUnderSampler(),\n",
    "# #     QuantileTransformer(),\n",
    "#     MinMaxScaler(),\n",
    "#     GridSearchCV(RandomForestClassifier(class_weight={True:3, False:1}),\n",
    "#                  param_grid=param_grid, \n",
    "#                  refit=True, \n",
    "#                  n_jobs=-1, \n",
    "#                  cv=2, \n",
    "#                  scoring='f1') \n",
    "# )\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    memory=None,\n",
    "    steps=[\n",
    "#         ('spl', SMOTE()),\n",
    "#         ('scl', MinMaxScaler()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "results = results.append({\n",
    "    'years_tracked': years_to_track, \n",
    "    'year_predicted': year_to_predict,\n",
    "    'score': pipeline.score(X=X_test, y=y_test),\n",
    "#     'f1': f1_score(y_pred=y_pred, y_true=y_test)\n",
    "}, ignore_index=True)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict_proba(df.loc[df['id'] == 29052283, cols_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_metric.functions import  BinaryClassification\n",
    "\n",
    "bc = BinaryClassification(y_test, y_prob, labels=[\"Low Impact\", \"High Impact\"])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\n",
    "bc.plot_roc_curve()\n",
    "\n",
    "plt.subplot2grid((2,6), (0,2), colspan=2)\n",
    "bc.plot_precision_recall_curve()\n",
    "\n",
    "plt.subplot2grid((2,6), (0,4), colspan=2)\n",
    "bc.plot_class_distribution()\n",
    "\n",
    "plt.subplot2grid((2,6), (1,1), colspan=2)\n",
    "bc.plot_confusion_matrix()\n",
    "\n",
    "plt.subplot2grid((2,6), (1,3), colspan=2)\n",
    "bc.plot_confusion_matrix(normalize=True)\n",
    "\n",
    "plt.show()\n",
    "bc.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "f1_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=results.pivot('years_tracked', 'year_predicted', 'score'), \n",
    "           annot=True, fmt='.2f', linewidth=.5, cbar=True, square=True, \n",
    "           cmap='YlGnBu', center=results['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "feature_importance = pd.DataFrame.from_dict(\n",
    "    dict(zip(X.columns,pipeline.steps[1][1].feature_importances_)), orient='index').T\n",
    "sns.barplot(orient='h',data=feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param = 'randomforestclassifier__max_depth'\n",
    "param_range = list(range(1,100,25))\n",
    "n_cv = 2\n",
    "\n",
    "train_scores, valid_scores = validation_curve(pipeline, \n",
    "                                              X=X, \n",
    "                                              y=y,\n",
    "                                              n_jobs=-1,\n",
    "                                              param_name=param,\n",
    "                                              scoring='roc_auc',\n",
    "                                              param_range=param_range,\n",
    "                                              cv=n_cv)\n",
    "\n",
    "vdf = pd.DataFrame(np.concatenate([train_scores, valid_scores]),\n",
    "             columns=['cv_fold_{}'.format(i) for i in range(n_cv)],\n",
    "            )\n",
    "vdf[param] = param_range*2\n",
    "vdf['type'] = ['train']*len(param_range) + ['valid']*len(param_range)\n",
    "\n",
    "sns.lineplot(data=vdf.melt(id_vars=['type', param]), \n",
    "             x=param, y='value', hue='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train, X_test]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2, n_jobs=-1, \n",
    "                                                    scoring='r2', config_dict='TPOT light', \n",
    "                                                    max_time_mins=30, max_eval_time_mins=5) \n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tpot_exported_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_ahead = 4\n",
    "df = pd.read_csv('/tmp/data/quanta.predict.{miny}.{maxy}.{ya}.csv.out'.format(\n",
    "        miny=min_year, maxy=max_year, ya=years_ahead))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlbox.preprocessing import *\n",
    "from mlbox.optimisation import *\n",
    "from mlbox.prediction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
