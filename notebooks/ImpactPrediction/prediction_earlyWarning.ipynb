{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, pickle\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# public_address = '54.174.175.98'\n",
    "public_address = '18.27.79.39'\n",
    "\n",
    "graph = Graph('bolt://{}:7687'.format(public_address), auth=('neo4j','myneo'))\n",
    "\n",
    "def run_query(query, graph, print_query=False, run_query=True, \n",
    "              print_only=False, to_df=False, verbose=True):\n",
    "    df = 1\n",
    "    if print_only: \n",
    "        print_query = True\n",
    "        run_query = False\n",
    "    start_time = time.time()\n",
    "    if print_query:\n",
    "        print(query)\n",
    "    if run_query:\n",
    "        if to_df:\n",
    "            df = graph.run(query).to_data_frame()\n",
    "        else:\n",
    "            graph.run(query)\n",
    "    end_time = time.time()\n",
    "    minutes_elapsed = (end_time-start_time)/60\n",
    "    if verbose:\n",
    "        print(\"Query completed in {:.2f} minutes.\".format(minutes_elapsed))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_consider = range(1950, 2020)\n",
    "years_tracked = 5\n",
    "\n",
    "vars_to_use = ['adopters', 'timeScaledPageRank', 'citatons', 'node2vec']\n",
    "\n",
    "max_year = max(years_to_consider)\n",
    "min_year = min(years_to_consider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data To CSV (With Author Features) Year By Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def get_quanta_features(years):\n",
    "    for year in years:\n",
    "        print('Running query for year {}'.format(year))\n",
    "        author_vars = [\n",
    "            'hIndex', 'hIndexDelta', 'totalCitations', 'totalCitationsDelta', 'citationsPerPaper', \n",
    "            'citationsPerPaperDelta','citationsPerYear', 'totalPapers', 'totalPapersDelta', 'rankCitationsPerYear',\n",
    "            'pageRank', 'authorAge', 'recentCoauthors', 'maxCitations', 'totalVenues', \n",
    "            'venueHIndexMean', 'venueHIndexDeltaMean', 'venueCitationsPerPaperMean', 'venueCitationsPerPaperDeltaMean',  \n",
    "            'venueTotalPapersMean', 'venueTotalPapersDeltaMean', 'venueRankCitationsPerPaperMean', 'venueMaxCitationsMean']\n",
    "\n",
    "\n",
    "        metrics_pattern = 'OPTIONAL MATCH (q)-[m{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "        metrics_string_list = [metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "        metrics_string = '\\n'.join(metrics_string_list)\n",
    "\n",
    "        where_pattern = 'exists(m{y}.node2vec)' #do we need to check all m{y}'s or just the first?\n",
    "        where_string_list = [where_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "        where_string = 'WHERE ' + ' AND '.join(where_string_list)   \n",
    "\n",
    "        # figure out how to only get q and y once\n",
    "        with_pattern = 'm{y}'\n",
    "        with_string_list = [with_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "        with_qy = 'WITH q, y, '\n",
    "        with_string_part = ', '.join(with_string_list)\n",
    "        with_string = with_qy + with_string_part\n",
    "\n",
    "        author_metrics_pattern = 'OPTIONAL MATCH (a)-[ma{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})' \n",
    "        author_metrics_string_list = [author_metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "        author_metrics_string = '\\n'.join(author_metrics_string_list)\n",
    "\n",
    "        with_pattern2_part1 = with_string + ', '\n",
    "        with_pattern2 = 'collect(ma{y}.{v}) as ma{y}_{v}'\n",
    "        with_string_list2 = [with_pattern2.format(y=i, v=j) for i in range(years_tracked+1) for j in author_vars]\n",
    "        with_pattern2_part2 = ', \\n '.join(with_string_list2)\n",
    "        with_string2 = with_pattern2_part1 + with_pattern2_part2\n",
    "\n",
    "\n",
    "        var_pattern = 'coalesce(m{y}.{v},0) AS {v}_{y},'\n",
    "        var_string_list = [var_pattern.format(y=i,v=j) for i in range(years_tracked+1) for j in vars_to_use]\n",
    "        var_string = '\\n'.join(var_string_list)\n",
    "\n",
    "        #author_funs = ['avg', 'max'] # are these the only things we want here?\n",
    "        #author_var_pattern = '{f}(coalesce(apoc.coll.avg(ma{y}_{v}), 0)) AS {v}_{f}_{y},'\n",
    "        author_var_pattern = 'coalesce(apoc.coll.avg(ma{y}_{v}), 0) AS {v}_{y}, \\n'\n",
    "\n",
    "        author_var_string_list = [author_var_pattern.format(y=i, v=j) #, f=k)\n",
    "                                  for i in range(years_tracked+1)\n",
    "                                  for j in author_vars]\n",
    "                                  #for k in author_funs]\n",
    "        author_var_string = ''.join(author_var_string_list)\n",
    "\n",
    "        query = \"\"\"\n",
    "        MATCH (q:Quanta)-[:PUBLISHED_IN]->(y:Year {{year:{the_year}}})\n",
    "        WITH q, y\n",
    "        {metrics_string}\n",
    "        {with_string}\n",
    "        WHERE exists(m0.node2vec)\n",
    "        MATCH (q)<-[:AUTHORED]-(a:Author)\n",
    "        {author_metrics_string}\n",
    "        {with_string2}\n",
    "        RETURN \n",
    "            {var_string}\n",
    "            {author_var_string}\n",
    "            id(q) AS id, \n",
    "            y.year AS year\n",
    "        \"\"\".format(the_year=year,\n",
    "                   metrics_string=metrics_string,\n",
    "                   where_string=where_string,\n",
    "                   with_string=with_string,\n",
    "                   with_string2=with_string2,\n",
    "                   var_string=var_string, \n",
    "                   author_metrics_string=author_metrics_string,\n",
    "                   author_var_string=author_var_string)\n",
    "\n",
    "        query_tocsv = \"\"\"\n",
    "        CALL apoc.export.csv.query('\n",
    "        {q}\n",
    "        ','/import/quanta.author.predict.{y}.csv', \n",
    "        {{quotes:true}});\n",
    "        \"\"\".format(q=query, \n",
    "                   y=year)\n",
    "\n",
    "        run_query(query_tocsv, graph, to_df=False, print_only=True)\n",
    "        \n",
    "get_quanta_features(years)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate CSV's into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "#os.chdir('/ltmp/data/')\n",
    "#quanta.author.predict\n",
    "extension = 'csv'\n",
    "all_files = [i for i in glob.glob('/ltmp/data/quanta.author.predict.*{}'.format(extension))]\n",
    "print(len(all_files))\n",
    "\n",
    "result = pd.read_csv(all_files[0])\n",
    "for f in tqdm(all_files[1:]):\n",
    "    result = pd.concat([result, pd.read_csv(f)], axis=0)\n",
    "    \n",
    "\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_files])\n",
    "combined_csv.to_csv(\"/ltmp/data/quanta.author.test.predict.allyrs.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data To CSV (With Author Features) All Years At Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data for each year, format it, and write it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fpath = '/tmp/data/quanta.predict.{miny}.{maxy}.{yt}.csv'.format(\n",
    "    #miny=min_year, maxy=max_year, yt=years_tracked, ya=years_ahead)\n",
    "fpath = '/tmp/data/quanta.author.predict.allyrs.csv'.format()\n",
    "df = pd.read_csv(fpath)\n",
    "df = df.dropna()\n",
    "\n",
    "#for y in tqdm(range(years_tracked+1)):\n",
    "for y in tqdm(range(11)):\n",
    "    col = 'node2vec_{}'.format(y)    \n",
    "    n2vdf = pd.DataFrame(df[col].apply(json.loads).tolist())\n",
    "    n2v_dim = n2vdf.shape[1]\n",
    "    n2vdf.columns = ['{}_{}'.format(col, i) for i in range(n2v_dim)]\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), n2vdf.reset_index(drop=True)], axis=1)\n",
    "    df = df.drop(col, axis=1)\n",
    "\n",
    "\n",
    "df.to_csv('{}.out'.format(fpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in prediction data and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, validation_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, mean_squared_error, classification_report, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/tmp/data/quanta.predict.{miny}.{maxy}.{yt}.csv.out'.format(\n",
    "    miny=min_year, maxy=max_year, yt=years_tracked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_tracked = 6\n",
    "\n",
    "results = []\n",
    "\n",
    "for years_to_track in tqdm(range(years_tracked)):\n",
    "    for year_to_predict in range(years_to_track+1, years_tracked+1):\n",
    "        year_to_predict = years_to_track + 1\n",
    "\n",
    "        cols_to_keep = ['{v}_{y}'.format(y=i,v=j) \n",
    "            for i in range(years_to_track+1) \n",
    "            for j in [v for v in vars_to_use if v!='node2vec']]\n",
    "\n",
    "        n2v_cols_to_keep =  ['node2vec_{y}_{i}'.format(y=y, i=i) \n",
    "                         for y in range(years_to_track+1)\n",
    "                         for i in range(n2v_dim)]\n",
    "\n",
    "        cols_to_keep = cols_to_keep + n2v_cols_to_keep\n",
    "        X = df.loc[:, cols_to_keep]\n",
    "\n",
    "        y_col = 'timeScaledPageRank_{y}'.format(y=year_to_predict)\n",
    "        y = df.loc[:, y_col] > df[y_col].quantile(q=.95)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            memory=None,\n",
    "            steps=[\n",
    "                ('spl', SMOTE()),\n",
    "                ('scl', QuantileTransformer()),\n",
    "                ('clf', RandomForestClassifier())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        grid = {'clf__n_estimators': [int(x) for x in np.linspace(200, 2000, num=10)],\n",
    "                'clf__max_features': ['auto', 'sqrt'],\n",
    "                'clf__max_depth': [int(x) for x in np.linspace(10, 1000, num=10)],\n",
    "                'clf__min_samples_split': [2, 5, 10],\n",
    "                'clf__min_samples_leaf': [1, 2, 4],\n",
    "                'clf__bootstrap': [True, False]}\n",
    "\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=pipeline, \n",
    "            param_distributions=grid, \n",
    "            n_iter=10, \n",
    "            cv=3, \n",
    "            n_jobs=-1,\n",
    "            verbose=3,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        with parallel_backend('threading'):\n",
    "            random_search.fit(X_train, y_train)\n",
    "        y_pred = random_search.predict(X_test)\n",
    "\n",
    "        results.append({\n",
    "            'years_tracked': years_to_track, \n",
    "            'year_predicted': year_to_predict,\n",
    "            'score': random_search.score(X=X_test, y=y_test),\n",
    "            'f1': f1_score(y_pred=y_pred, y_true=y_test),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n",
    "            'balanced_accuracy_adjusted': balanced_accuracy_score(y_true=y_test, y_pred=y_pred, adjusted=True),\n",
    "            'classification_report': classification_report(y_true=y_test, y_pred=y_pred, output_dict=True),\n",
    "            'random_search': random_search\n",
    "        })\n",
    "\n",
    "        pickle.dump(results, open('predmodel_{yt}_{yp}.pickle'.format(\n",
    "            yt=years_to_track, yp=year_to_predict),'wb'))    \n",
    "pickle.dump(results, open('predmodels_{yst}.pickle'.format(yst=years_tracked),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('/tmp/data/predmodels_{yst}.pickle'.format(yst=years_tracked),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_metric.functions import  BinaryClassification\n",
    "\n",
    "bc = BinaryClassification(y_test, y_prob, labels=[\"Low Impact\", \"High Impact\"])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\n",
    "bc.plot_roc_curve()\n",
    "\n",
    "plt.subplot2grid((2,6), (0,2), colspan=2)\n",
    "bc.plot_precision_recall_curve()\n",
    "\n",
    "plt.subplot2grid((2,6), (0,4), colspan=2)\n",
    "bc.plot_class_distribution()\n",
    "\n",
    "plt.subplot2grid((2,6), (1,1), colspan=2)\n",
    "bc.plot_confusion_matrix()\n",
    "\n",
    "plt.subplot2grid((2,6), (1,3), colspan=2)\n",
    "bc.plot_confusion_matrix(normalize=True)\n",
    "\n",
    "plt.show()\n",
    "bc.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "f1_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=results.pivot('years_tracked', 'year_predicted', 'score'), \n",
    "           annot=True, fmt='.2f', linewidth=.5, cbar=True, square=True, \n",
    "           cmap='YlGnBu', center=results['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "feature_importance = pd.DataFrame.from_dict(\n",
    "    dict(zip(X.columns,pipeline.steps[1][1].feature_importances_)), orient='index').T\n",
    "sns.barplot(orient='h',data=feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param = 'randomforestclassifier__max_depth'\n",
    "param_range = list(range(1,100,25))\n",
    "n_cv = 2\n",
    "\n",
    "train_scores, valid_scores = validation_curve(pipeline, \n",
    "                                              X=X, \n",
    "                                              y=y,\n",
    "                                              n_jobs=-1,\n",
    "                                              param_name=param,\n",
    "                                              scoring='roc_auc',\n",
    "                                              param_range=param_range,\n",
    "                                              cv=n_cv)\n",
    "\n",
    "vdf = pd.DataFrame(np.concatenate([train_scores, valid_scores]),\n",
    "             columns=['cv_fold_{}'.format(i) for i in range(n_cv)],\n",
    "            )\n",
    "vdf[param] = param_range*2\n",
    "vdf['type'] = ['train']*len(param_range) + ['valid']*len(param_range)\n",
    "\n",
    "sns.lineplot(data=vdf.melt(id_vars=['type', param]), \n",
    "             x=param, y='value', hue='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train, X_test]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2, n_jobs=-1, \n",
    "                                                    scoring='r2', config_dict='TPOT light', \n",
    "                                                    max_time_mins=30, max_eval_time_mins=5) \n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tpot_exported_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_ahead = 4\n",
    "df = pd.read_csv('/tmp/data/quanta.predict.{miny}.{maxy}.{ya}.csv.out'.format(\n",
    "        miny=min_year, maxy=max_year, ya=years_ahead))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlbox.preprocessing import *\n",
    "from mlbox.optimisation import *\n",
    "from mlbox.prediction import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to CSV (just quanta features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     32,
     37,
     51
    ]
   },
   "outputs": [],
   "source": [
    "metrics_pattern = 'MATCH (q)-[m{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "metrics_string_list = [metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "metrics_string = '\\n'.join(metrics_string_list)\n",
    "\n",
    "var_pattern = 'coalesce(m{y}.{v},0) AS {v}_{y},'\n",
    "var_string_list = [var_pattern.format(y=i,v=j) for i in range(years_tracked+1) for j in vars_to_use]\n",
    "var_string = '\\n'.join(var_string_list)\n",
    "\n",
    "where_pattern = 'exists(m{y}.node2vec)'\n",
    "where_string_list = [where_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "where_string = ' AND '.join(where_string_list)       \n",
    "\n",
    "author_metrics_pattern = 'MATCH (q)-[ma{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "author_metrics_string_list = [author_metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "author_metrics_string = '\\n'.join(author_metrics_string_list)\n",
    "\n",
    "author_vars = ['total_papers', 'author_age', 'max_citations', 'num_venues', 'total_citations']\n",
    "author_funs = ['max', 'sum']\n",
    "author_var_pattern = '{f}(coalesce(ma{y}.{v},0)) AS {v}_{f}_{y},'\n",
    "author_var_string_list = [author_var_pattern.format(y=i, v=j, f=k)\n",
    "                          for i in range(years_tracked+1)\n",
    "                          for j in author_vars\n",
    "                          for k in author_funs]\n",
    "author_var_string = '\\n'.join(author_var_string_list)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Author)-[:AUTHORED]->(q:Quanta)-[:PUBLISHED_IN]->(y:Year)\n",
    "WHERE y.year>={miny} AND y.year<={maxy}\n",
    "{metrics_string}\n",
    "{author_metrics_string}\n",
    "WHERE {where_string}\n",
    "RETURN \n",
    "    {var_string}\n",
    "    {author_var_string}\n",
    "    id(q) AS id, \n",
    "    y.year AS year\n",
    "\"\"\".format(miny=min_year, \n",
    "           maxy=max_year,\n",
    "           metrics_string=metrics_string,\n",
    "           var_string=var_string, \n",
    "           where_string=where_string, \n",
    "           author_metrics_string='',\n",
    "           author_var_string='')\n",
    "\n",
    "query_tocsv = \"\"\"\n",
    "CALL apoc.export.csv.query('\n",
    "{q}\n",
    "','/import/quanta.predict.{miny}.{maxy}.csv', \n",
    "{{quotes:true}});\n",
    "\"\"\".format(q=query, \n",
    "           miny=min_year, \n",
    "           maxy=max_year,\n",
    "           yt=years_tracked)\n",
    "\n",
    "run_query(query_tocsv, graph, to_df=False, print_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def write_features_all_years():\n",
    "    #author_vars = ['total_papers', 'author_age', 'max_citations', 'num_venues', 'total_citations']\n",
    "    # todo add weighted_pagerank\n",
    "    author_vars = ['hIndex', 'totalPapers', 'authorAge', 'maxCitations', 'totalCitations', 'numVenues', 'pageRank']\n",
    "\n",
    "    author_vars_all = [\n",
    "        'hIndex', 'hIndexDelta', 'totalCitations', 'totalCitationsDelta', 'citationsPerPaper', \n",
    "        'citationsPerPaperDelta','citationsPerYear', 'totalPapers', 'totalPapersDelta', 'rankCitationsPerYear',\n",
    "        'pageRank', 'authorAge', 'recentCoauthors', 'maxCitations', 'totalVenues', \n",
    "        'venueHIndexMean', 'venueHIndexDeltaMean', 'venueCitationsPerPaperMean', 'venueCitationsPerPaperDeltaMean',  \n",
    "        'venueTotalPapersMean', 'venueTotalPapersDeltaMean', 'venueRankCitationsPerPaperMean', 'venueMaxCitationsMean']\n",
    "\n",
    "\n",
    "    metrics_pattern = 'MATCH (q)-[m{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})'\n",
    "    metrics_string_list = [metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "    metrics_string = '\\n'.join(metrics_string_list)\n",
    "\n",
    "    where_pattern = 'exists(m{y}.node2vec)' #do we need to check all m{y}'s or just the first?\n",
    "    where_string_list = [where_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "    where_string = 'WHERE ' + ' AND '.join(where_string_list)   \n",
    "\n",
    "    # figure out how to only get q and y once\n",
    "    with_pattern = 'm{y}'\n",
    "    with_string_list = [with_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "    with_qy = 'WITH q, y, '\n",
    "    with_string_part = ', '.join(with_string_list)\n",
    "    with_string = with_qy + with_string_part\n",
    "\n",
    "    author_metrics_pattern = 'MATCH (a)-[ma{y}:METRICS_IN]->(:Year {{year:y.year+{y}}})' \n",
    "    author_metrics_string_list = [author_metrics_pattern.format(y=i) for i in range(years_tracked+1)]\n",
    "    author_metrics_string = '\\n'.join(author_metrics_string_list)\n",
    "\n",
    "    # figure out how to only get q and y once\n",
    "    #with_pattern2 = 'WITH q, y, m{y}, collect(ma{y}.{v}) as ma{y}_{v} '\n",
    "    #with_string2 = '\\n'.join(with_string_list)\n",
    "    with_pattern2_part1 = with_string + ', '\n",
    "    with_pattern2 = 'collect(ma{y}.{v}) as ma{y}_{v}'\n",
    "    with_string_list2 = [with_pattern2.format(y=i, v=j) for i in range(years_tracked+1) for j in author_vars]\n",
    "    with_pattern2_part2 = ', \\n '.join(with_string_list2)\n",
    "    with_string2 = with_pattern2_part1 + with_pattern2_part2\n",
    "\n",
    "\n",
    "    var_pattern = 'coalesce(m{y}.{v},0) AS {v}_{y},'\n",
    "    var_string_list = [var_pattern.format(y=i,v=j) for i in range(years_tracked+1) for j in vars_to_use]\n",
    "    var_string = '\\n'.join(var_string_list)\n",
    "\n",
    "    #author_funs = ['avg', 'max'] # are these the only things we want here?\n",
    "    #author_var_pattern = '{f}(coalesce(apoc.coll.avg(ma{y}_{v}), 0)) AS {v}_{f}_{y},'\n",
    "    author_var_pattern = 'coalesce(apoc.coll.avg(ma{y}_{v}), 0) AS {v}_{y}, \\n'\n",
    "\n",
    "    author_var_string_list = [author_var_pattern.format(y=i, v=j) #, f=k)\n",
    "                              for i in range(years_tracked+1)\n",
    "                              for j in author_vars]\n",
    "                              #for k in author_funs]\n",
    "    author_var_string = ''.join(author_var_string_list)\n",
    "\n",
    "    query = \"\"\"\n",
    "    MATCH (q:Quanta)-[:PUBLISHED_IN]->(y:Year)\n",
    "    WHERE y.year>={miny} AND y.year<={maxy}\n",
    "    WITH q, y\n",
    "    {metrics_string}\n",
    "    {with_string}\n",
    "    {where_string} \n",
    "    MATCH (q)<-[:AUTHORED]-(a:Author)\n",
    "    {author_metrics_string}\n",
    "    {with_string2}\n",
    "    RETURN \n",
    "        {var_string}\n",
    "        {author_var_string}\n",
    "        id(q) AS id, \n",
    "        y.year AS year\n",
    "    \"\"\".format(miny=min_year, \n",
    "               maxy=max_year,\n",
    "               metrics_string=metrics_string,\n",
    "               where_string=where_string,\n",
    "               with_string=with_string,\n",
    "               with_string2=with_string2,\n",
    "               var_string=var_string, \n",
    "               author_metrics_string=author_metrics_string,\n",
    "               author_var_string=author_var_string)\n",
    "\n",
    "    query_tocsv = \"\"\"\n",
    "    CALL apoc.export.csv.query('\n",
    "    {q}\n",
    "    ','/import/quanta.author.predict.{miny}.{maxy}.csv', \n",
    "    {{quotes:true}});\n",
    "    \"\"\".format(q=query, \n",
    "               miny=min_year, \n",
    "               maxy=max_year,\n",
    "               yt=years_tracked)\n",
    "\n",
    "    run_query(query_tocsv, graph, to_df=False, print_only=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
