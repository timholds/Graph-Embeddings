{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert H5 files to CSV for Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "data_files = glob.glob('/tmp/data/patents/*.h5')\n",
    "for file in data_files:\n",
    "    \n",
    "    print(\"Reading {}...\".format(file), end=\" \")\n",
    "    df = pd.read_hdf(file)\n",
    "    df.columns = ['title','inventors','pub_date','classifications','n_citations',\n",
    "                  'cited_by', 'patent_citations','also_published_as','location']\n",
    "    df['title'] = df['title'].str.strip()\n",
    "    df['classifications'] = df['classifications'].str.join(\";\")\n",
    "    df['inventors'] = df['inventors'].str.join(\";\")\n",
    "    df['cited_by'] = df['cited_by'].str.join(\";\")\n",
    "    df['patent_citations'] = df['patent_citations'].str.join(\";\")\n",
    "    df['also_published_as'] = df['also_published_as'].str.join(\";\")\n",
    "#     df.replace(\"\", np.nan, inplace=True)\n",
    "    assert(not df.index.duplicated().any())\n",
    "    \n",
    "    new_file = file.replace('.h5','.csv')\n",
    "    print(\"Writing {}...\".format(new_file), end=\" \")\n",
    "    df.to_csv(path_or_buf=new_file, sep=',', quoting=csv.QUOTE_MINIMAL, chunksize=50000, index_label='number', na_rep='na')\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to do something with the ALSO published as\n",
    "# sum(df['also_published_as'].str.len())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "# Need to get authentication working, currently NEO4J_AUTH=none\n",
    "graph = Graph(\"bolt://neo4j:7687\")\n",
    "\n",
    "#graph = Graph(\"bolt://matlaber5.media.mit.edu:7687\")\n",
    "\n",
    "\n",
    "#graph.delete_all()\n",
    "\n",
    "n_nodes = graph.database.primitive_counts['NumberOfNodeIdsInUse']\n",
    "n_relationships = graph.database.primitive_counts['NumberOfRelationshipIdsInUse']\n",
    "print(\"Connected to graph database with {:,} nodes and {:,} relationships!\".format\n",
    "     (n_nodes, n_relationships))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constrain one id per Patent\n",
    "# print(\"Creating uniqueness constraint (and also index) on Patent numbers...\", end=\" \", flush=True)\n",
    "# query = \"\"\"CREATE CONSTRAINT ON (n:Patent) ASSERT n.number IS UNIQUE;\"\"\"\n",
    "# graph.run(query).evaluate()\n",
    "# print(\"Done.\")\n",
    "\n",
    "# Add index for number\n",
    "print(\"Creating uniqueness constraint for id...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE CONSTRAINT ON (n:Patent) ASSERT n.id IS UNIQUE;\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Add index for classifications\n",
    "print(\"Creating index for classifications...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE INDEX ON :Patent(classifications);\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# # Add index for inventors\n",
    "# print(\"Creating index for inventors...\", end=\" \", flush=True)\n",
    "# query = \"\"\"CREATE INDEX ON :Patent(inventors);\"\"\"\n",
    "# graph.run(query).evaluate()\n",
    "# print(\"Done.\")\n",
    "\n",
    "# Add index for location\n",
    "print(\"Creating index for location...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE INDEX ON :Patent(location);\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Add index for pub_date\n",
    "print(\"Creating index for pub_date...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE INDEX ON :Patent(pub_date);\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Creating uniqueness constraint for author...\", end=\" \", flush=True)\n",
    "query = \"\"\"CREATE CONSTRAINT ON (a:Author) ASSERT a.name IS UNIQUE;\"\"\"\n",
    "graph.run(query).evaluate()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import patents\n",
    "\n",
    "local_data_dir = '/tmp/data/patents/'\n",
    "neo4j_data_dir = '/import/patents/'\n",
    "\n",
    "import glob, os, time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _, _, files in os.walk(local_data_dir):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith('.csv'):\n",
    "            print(\"Importing {}...\".format(file), end=\" \", flush=True)\n",
    "            query_start_time = time.time()\n",
    "            query = \"\"\"\n",
    "            CALL apoc.periodic.iterate(\"\n",
    "            CALL apoc.load.csv(\n",
    "                'file://{}{}',\n",
    "                {{\n",
    "                 header:true,sep:',',\n",
    "                 mapping:{{\n",
    "                  number:{{name:'id'}},\n",
    "                  patent_citations:{{name:'refs',array:true,arraySep:';'}},\n",
    "                  also_published_as:{{array:true,arraySep:';'}},\n",
    "                  cited_by:{{array:true,arraySep:';'}},\n",
    "                  inventors:{{name:'authors',array:true,arraySep:';'}},\n",
    "                  n_citations:{{name:'n_citation',type:'int',arraySep:';'}},\n",
    "                  classifications:{{array:true,arraySep:';'}},\n",
    "                  location:{{}}\n",
    "                }}\n",
    "            }}) YIELD map as row \n",
    "            RETURN row\n",
    "            \",\n",
    "            \"CREATE (p:Patent) SET p = row\n",
    "                SET p.patent_citations = [f IN p.patent_citations WHERE f <> '']\n",
    "                SET p.also_published_as = [f IN p.also_published_as WHERE f <> '']\n",
    "                SET p.cited_by = [f IN p.cited_by WHERE f <> '']\n",
    "                SET p.authors = [f IN p.authors WHERE f <> '']\n",
    "                SET p.classifications = [f IN p.classifications WHERE f <> '']\n",
    "                SET p.year = toInt(head(split(p.pub_date,'-')))\n",
    "            \", \n",
    "            {{batchsize:10000, iterateList:true, parallel:false}}\n",
    "            );\n",
    "            \"\"\".format(neo4j_data_dir, file)\n",
    "#             print(query)\n",
    "            graph.run(query).evaluate()\n",
    "            query_end_time = time.time()\n",
    "            print(\"Done ({:.2f} minutes).\".format((query_end_time-query_start_time)/60))\n",
    "            \n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating AUTHORED relationships...\", end=\" \")\n",
    "start_time = time.time()\n",
    "query = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "\"MATCH (p:Patent) UNWIND p.authors AS auth  RETURN p, auth\"\n",
    ",\n",
    "\"MERGE (a:Author {name: auth}) \n",
    "MERGE (a)-[:AUTHORED {is_first_author: head(p.authors)=auth, is_last_author: last(p.authors)=auth}]->(p)\"\n",
    ",\n",
    "{batchSize:500, iterateList:true, parallel:false})\n",
    "\"\"\"\n",
    "\n",
    "# # other option\n",
    "# query = \"\"\"\n",
    "# CALL apoc.periodic.commit(\n",
    "# \"MATCH (p:Patent) \n",
    "# UNWIND p.authors AS auth\n",
    "# WITH p, auth\n",
    "# MERGE (a:Author {name: auth}) \n",
    "# MERGE (a)-[:AUTHORED {is_first_author: head(p.authors)=auth, is_last_author: last(p.authors)=auth}]->(p)\"\n",
    "# ,\n",
    "# {limit:1000})\n",
    "# \"\"\"\n",
    "\n",
    "#print(query)\n",
    "graph.run(query).evaluate()\n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding CITES relationships...\", end=\" \", flush=True)\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "query = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "\"MATCH (a:Patent) UNWIND a.cited_by AS ref RETURN a, ref\",\n",
    "\"MATCH (b:Patent {id: ref}) \n",
    "MERGE (b)-[:CITES]->(a)\",\n",
    "{batchSize:200, iterateList:true, parallel:false})\n",
    "\"\"\"\n",
    "\n",
    "# query = \"\"\"\n",
    "# CALL apoc.periodic.commit(\n",
    "# \"MATCH (p:Patent) UNWIND p.cited_by AS ref\n",
    "# WITH p, ref\n",
    "# MATCH (b:Patent {id: ref}) \n",
    "# MERGE (b)-[:CITES]->(p)\"\n",
    "# ,\n",
    "# {limit:1000})\n",
    "# \"\"\"\n",
    "\n",
    "#print(query)\n",
    "graph.run(query).evaluate()\n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = graph.database.primitive_counts['NumberOfNodeIdsInUse']\n",
    "n_relationships = graph.database.primitive_counts['NumberOfRelationshipIdsInUse']\n",
    "print(\"Built a graph database with {:,} nodes and {:,} relationships!\".format\n",
    "     (n_nodes, n_relationships))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PageRank once on entire graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "    \n",
    "print(\"Running PageRank on all patents...\", end=\" \")\n",
    "# query = \"\"\"\n",
    "# CALL algo.pageRank.stream(\n",
    "#     'MATCH (p:Patent) RETURN id(p) as id'\n",
    "#     ,\n",
    "#     'MATCH (p1:Patent)-[:CITES]->(p2:Patent) RETURN id(p1) as source, id(p2) as target'\n",
    "#     ,\n",
    "#     {graph:'cypher', iterations:20, write:true, writeProperty:'pagerank'}\n",
    "#     )\n",
    "# YIELD node, score\n",
    "# WITH * \n",
    "# ORDER BY score DESC\n",
    "# LIMIT 50\n",
    "# RETURN node.number AS number, \n",
    "#     node.title AS title, \n",
    "#     node.inventors AS inventors, \n",
    "#     node.location AS location, \n",
    "#     node.n_citations AS n_citations, \n",
    "#     node.pub_date AS pub_date, \n",
    "#     node.patent_citations AS patent_citations, \n",
    "#     node.classifications AS classifications, \n",
    "#     node.cited_by AS cited_by, \n",
    "#     score;\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "CALL algo.pageRank(\n",
    "    'MATCH (p:Patent) RETURN id(p) as id'\n",
    "    ,\n",
    "    'MATCH (p1:Patent)-[:CITES]->(p2:Patent) RETURN id(p1) as source, id(p2) as target'\n",
    "    ,\n",
    "    {graph:'cypher', iterations:30, write:true, writeProperty:'pagerank'}\n",
    "    );\n",
    "\"\"\"\n",
    "#print(query)\n",
    "df = graph.run(query).to_data_frame()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most influential patents and inventors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Finding Patents with highest PageRank...\", end=\" \")\n",
    "query = \"\"\"\n",
    "MATCH (a:Author)-[:AUTHORED]->(p:Patent)\n",
    "RETURN \n",
    "a.name AS name, \n",
    "COUNT(p) AS patents,\n",
    "SUM(p.n_citation) AS citations, \n",
    "SUM(p.pagerank) AS pagerank\n",
    "ORDER BY pagerank DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df_patents = graph.run(query).to_data_frame()\n",
    "end_time = time.time()\n",
    "print(\"Done in {:.2f} minutes.\".format((end_time-start_time)/60))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Finding Authors with highest PageRank...\", end=\" \")\n",
    "query = \"\"\"\n",
    "MATCH (a:Author)-[:AUTHORED]->(p:Patent)\n",
    "RETURN \n",
    "a.name AS name, \n",
    "COUNT(p) AS patents,\n",
    "SUM(p.n_citation) AS citations, \n",
    "SUM(p.pagerank) AS pagerank\n",
    "ORDER BY pagerank DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df_authors = graph.run(query).to_data_frame()\n",
    "end_time = time.time()\n",
    "print(\"Done in {:.2f} minutes.\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PageRank over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run STREAMING PageRank on each year from 1800 to 1805\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "start_year, end_year, step = 1900, 2020, 5\n",
    "dfs = []\n",
    "\n",
    "for year in range(start_year, end_year+1, step):\n",
    "    \n",
    "    print(\"Considering patents from < {}\\n\\tRunning PageRank...\".format(year), end=\" \")\n",
    "    query = \"\"\"\n",
    "    CALL algo.pageRank(\n",
    "        'MATCH (a:Patent)-[:CITES]->(b:Patent) WHERE a.year < {} OR b.year < {} RETURN id(a) as id'\n",
    "        ,\n",
    "        'MATCH (p1:Patent)-[:CITES]->(p2:Patent) RETURN id(p1) as source, id(p2) as target'\n",
    "        ,\n",
    "        {{graph:'cypher', iterations:30, write:true, writeProperty:'pagerank_yr'}}\n",
    "        );\n",
    "    \"\"\".format(year,year)\n",
    "#     print(query)\n",
    "    query_start_time = time.time()\n",
    "    df = graph.run(query).to_data_frame()\n",
    "    print(\"Finished ({:.2f} minutes).\".format((time.time()-start_time)/60))    \n",
    "    \n",
    "    print(\"\\tGetting author impact scores...\".format(year), end=\" \")\n",
    "    query = \"\"\"\n",
    "    MATCH (a:Author)-[:AUTHORED]->(p:Patent)\n",
    "    WHERE p.year < {}\n",
    "    RETURN \n",
    "        a.name AS name, \n",
    "        COUNT(p) AS patents,\n",
    "        SUM(p.n_citation) AS citations, \n",
    "        SUM(p.pagerank_yr) AS pagerank,\n",
    "        SUM(LOG(p.pagerank_yr*exp(12*(2018-p.year)/-700))) AS impact\n",
    "    ORDER BY impact DESC\n",
    "    LIMIT 50000\n",
    "    \"\"\".format(year)\n",
    "#     print(query)\n",
    "    query_start_time = time.time()\n",
    "    df = graph.run(query).to_data_frame()\n",
    "    dfs.append(df)\n",
    "    print(\"Finished ({:.2f} minutes).\".format((time.time()-query_start_time)/60))    \n",
    "    end_time = time.time()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, year in enumerate(range(start_year, end_year+1, step)):\n",
    "    dfs[i]['year'] = year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat(dfs)\n",
    "all_results_path = '/tmp/data/result/patents_impact_{}-{}-{}.csv'.format(start_year, end_year,step)\n",
    "all_results.set_index('name', inplace=True)\n",
    "\n",
    "print(\"Writing all results to {}...\".format(all_results_path), end=\" \")\n",
    "all_results.to_csv(path_or_buf=all_results_path, sep=\",\", header=True, index=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pivot\n",
    "result = all_results.pivot_table(index='name', columns='year', values='pagerank')    \n",
    "\n",
    "# Remove some inventors that appear and then dissappear in the top list\n",
    "\n",
    "first_valid_index_vec = result.apply(lambda x: x.first_valid_index(), axis=1)\n",
    "last_valid_index_vec = result.apply(lambda x: x.last_valid_index(), axis=1)\n",
    "\n",
    "start_mask = list(first_valid_index_vec<=1970)\n",
    "end_mask = list(last_valid_index_vec==end_year)\n",
    "full_mask = [(a and b) for a, b in zip(start_mask, end_mask)]\n",
    "cleaned_results = result.iloc[full_mask,:]\n",
    "\n",
    "# Write out to file\n",
    "file_path = '/tmp/data/result/patents_pivottable_{}-{}-{}.csv'.format(start_year, end_year,step)\n",
    "print(\"Writing results to {}...\".format(file_path), end=\" \")\n",
    "cleaned_results.to_csv(path_or_buf=file_path, sep=\",\", header=True, index=True)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Using {:,} authors in subsequent calculations.\".format(cleaned_results.shape[0]))\n",
    "cleaned_results.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate optimal portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data and calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = cleaned_results.pct_change(periods=1, axis=1).T\n",
    "mean_returns = returns.mean()\n",
    "std_returns = returns.std()\n",
    "cov_matrix = returns.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sorted_by_std = std_returns.sort_values(axis=0, ascending=False)\n",
    "# high_std_index = sorted_by_std.index[1000]\n",
    "# sorted_by_mean = mean_returns.sort_values(axis=0, ascending=False)\n",
    "# high_mean_index = sorted_by_mean.index[1000]\n",
    "# plt.plot(returns[high_std_index],'b')\n",
    "# plt.plot(returns[high_mean_index],'k')\n",
    "\n",
    "min_val = 10e-3\n",
    "returns.clip_lower(min_val, inplace=True)\n",
    "returns.replace(to_replace=min_val, value=None, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.semilogy(returns)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Impact')\n",
    "plt.title('Impact mean and variance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Mean and Variance', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_portfolios = 10000\n",
    "n_assets = returns.shape[1]\n",
    "\n",
    "output = np.zeros((3,num_portfolios))\n",
    "for i in range(num_portfolios):\n",
    "    \n",
    "    n_assets_to_include = n_assets\n",
    "    risk_free = 0\n",
    "    \n",
    "    w = np.zeros(returns.shape[1])\n",
    "    w[np.random.choice(w.shape[0],n_assets_to_include)] = np.random.random(n_assets_to_include)\n",
    "    w /= np.sum(w)\n",
    "    \n",
    "    p_return = np.sum(mean_returns*w)\n",
    "    p_stdev = np.sqrt(np.dot(w.T, np.dot(cov_matrix, w)))\n",
    "    \n",
    "    # TODO: Convert to annual basis (currently 5 years)\n",
    "    output[0,i] = p_return\n",
    "    output[1,i] = p_stdev\n",
    "    output[2,i] = (output[0,i] - risk_free)/output[1,i]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(output[1,:], output[0,:], c=output[2,:], marker='o')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('Expected Variance')\n",
    "plt.ylabel('Expected Impact')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.title('Monte Carlo Simulation of {:,} {}-Person Grants'.format(num_portfolios, n_assets_to_include))\n",
    "plt.tight_layout()\n",
    "plt.savefig('Monte Carlo Simulation', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as sco\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top patents in Boston/MA/Cambridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()    \n",
    "print(\"Getting top Boston/MA/Cambridge inventors and patents...\", end=\" \")\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Author)-[:AUTHORED]->(p:Patent)\n",
    "WHERE \n",
    "    p.location =~ \"(?i).*[, ]?(boston|massachusetts)[, ]?.*\" OR\n",
    "    p.location =~ \"(?i).*[, ]?(rhode island|new hampshire)[, ]?.*\" OR    \n",
    "    p.location =~ \"(?i).*[, ]+(ma|ri|n)[, ]+.*\" \n",
    "RETURN \n",
    "    a.name AS name, \n",
    "    p.location,\n",
    "    COUNT(p) AS patents,\n",
    "    SUM(p.n_citation) AS citations, \n",
    "    SUM(p.pagerank) AS pagerank,\n",
    "    SUM(LOG(p.pagerank*exp(12*(2018-p.year)/-700))) AS impact\n",
    "ORDER BY pagerank DESC\n",
    "\"\"\"\n",
    "df = graph.run(query).to_data_frame()\n",
    "end_time = time.time()\n",
    "print(\"Finished all calculations in {:.2f} minutes.\".format((end_time-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['name', 'pagerank','impact']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasify and rank patents by author gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, random\n",
    "from nltk.corpus import names as nltk_names\n",
    "\n",
    "nltk.download(\"names\")\n",
    "male_names = list(nltk_names.words('male.txt'))\n",
    "female_names = list(nltk_names.words('female.txt'))\n",
    "\n",
    "labeled_names = [({\"name\": name}, 'male') for name in male_names]\n",
    "labeled_names.extend([({\"name\": name}, 'female') for name in female_names])\n",
    "random.shuffle(labeled_names)\n",
    "classifier = nltk.NaiveBayesClassifier.train(labeled_names)\n",
    "\n",
    "print(\"Trained classifier with {:,} names ({:.2f}% male, {:.2f}% female)\".format(\n",
    "    len(labeled_names), \n",
    "    len(male_names)/len(labeled_names)*100, \n",
    "    len(female_names)/len(labeled_names)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_inventors = []\n",
    "female_prob_threshold = .75\n",
    "\n",
    "for name,score in zip(df['name'],df['pagerank']):\n",
    "    split_name = name.split(' ')\n",
    "    if len(split_name)>1 and len(split_name)<4 and len(split_name[0])>1:\n",
    "        first_name = split_name[0]\n",
    "        if classifier.prob_classify({\"name\": first_name}).prob('female') > female_prob_threshold:\n",
    "            female_inventors.append((name,score))\n",
    "print(\"Found {:,} female inventors\".format(len(female_inventors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/tmp/data/result/BostonMostImpactfulFemaleInventors.csv'\n",
    "\n",
    "score_threshold = 1\n",
    "top_female_inventors = [t for t in female_inventors if t[1]>score_threshold]\n",
    "print(\"Filtered down to the top {:,} female inventors\".format(len(top_female_inventors)))\n",
    "\n",
    "with open(file_path,'w') as file:\n",
    "    file.write('Name, Impact Score\\n')\n",
    "    for name,score in top_female_inventors:\n",
    "        print(\"{}, Impact Score: {:,.2f}\".format(name,score))\n",
    "        file.write('{}, {:.2f}\\n'.format(name, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
